{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8b3a9f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Jack Wilson\n",
    "9/23/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7868b9",
   "metadata": {},
   "source": [
    "This notebook outlines scraping and collecting of all data raw data used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e71531",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8a9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, random, re, os, gc, shutil, pickle, tempfile, shutil\n",
    "from math import e\n",
    "\n",
    "import fastf1\n",
    "import logging\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7736f",
   "metadata": {},
   "source": [
    "# DataFrame Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fc566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7e249",
   "metadata": {},
   "source": [
    "# Functions and Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a0ea2",
   "metadata": {},
   "source": [
    "## Constructor Common Name Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ac7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructors common name mapping\n",
    "constructor_mapping = {'team_id': {\n",
    "    # Red Bull\n",
    "    'Red Bull Racing Renault': 'Red Bull',\n",
    "    'Red Bull Renault': 'Red Bull',\n",
    "    'RBR Renault': 'Red Bull',\n",
    "    'RBR Cosworth': 'Red Bull',\n",
    "    'RBR Ferrari': 'Red Bull',\n",
    "    'Red Bull Racing TAG Heuer': 'Red Bull',\n",
    "    'Red Bull Racing Honda': 'Red Bull',\n",
    "    'Red Bull Racing RBPT': 'Red Bull',\n",
    "    'Red Bull Racing Honda RBPT': 'Red Bull',\n",
    "    'Red Bull Racing': 'Red Bull',\n",
    "    \n",
    "    # AlphaTauri/Toro Rosso\n",
    "    'Toro Rosso': 'Toro Rosso',\n",
    "    'STR Ferrari': 'Toro Rosso',\n",
    "    'STR Renault': 'Toro Rosso',\n",
    "    'STR Cosworth': 'Toro Rosso',\n",
    "    'Toro Rosso Ferrari': 'Toro Rosso',\n",
    "    'Scuderia Toro Rosso Honda': 'Toro Rosso',\n",
    "    'AlphaTauri Honda': 'AlphaTauri',\n",
    "    'AlphaTauri RBPT': 'AlphaTauri',\n",
    "    'AlphaTauri Honda RBPT': 'AlphaTauri',\n",
    "    \n",
    "    # Racing Bulls\n",
    "    'RB Honda RBPT': 'Racing Bulls',\n",
    "    \n",
    "    # Ferrari\n",
    "    'Ferrari': 'Ferrari',\n",
    "    'Ferrari Jaguar': 'Ferrari',\n",
    "    'Thin Wall Ferrari': 'Ferrari',\n",
    "    \n",
    "    # Mercedes\n",
    "    'Mercedes': 'Mercedes',\n",
    "    'Mercedes-Benz': 'Mercedes',\n",
    "    \n",
    "    # Aston Martin\n",
    "    'Aston Martin Mercedes': 'Aston Martin',\n",
    "    'Aston Martin Aramco Mercedes': 'Aston Martin',\n",
    "    'Aston Butterworth': 'Aston Martin',\n",
    "    'Aston Martin': 'Aston Martin',\n",
    "    \n",
    "    # McLaren\n",
    "    'McLaren Ford': 'McLaren',\n",
    "    'McLaren TAG': 'McLaren',\n",
    "    'McLaren Honda': 'McLaren',\n",
    "    'McLaren Peugeot': 'McLaren',\n",
    "    'McLaren Renault': 'McLaren',\n",
    "    'McLaren BRM': 'McLaren',\n",
    "    'McLaren Mercedes': 'McLaren',\n",
    "    'McLaren Serenissima': 'McLaren',\n",
    "    'Mclaren BRM': 'McLaren',\n",
    "    'McLaren Alfa Romeo': 'McLaren',\n",
    "    \n",
    "    # Williams\n",
    "    'Williams Ford': 'Williams',\n",
    "    'Williams Renault': 'Williams',\n",
    "    'Williams Honda': 'Williams',\n",
    "    'Williams Judd': 'Williams',\n",
    "    'Williams BMW': 'Williams',\n",
    "    'Williams Toyota': 'Williams',\n",
    "    'Williams Cosworth': 'Williams',\n",
    "    'Williams Mecachrome': 'Williams',\n",
    "    'Williams Supertec': 'Williams',\n",
    "    'Williams Mercedes': 'Williams',\n",
    "    'Frank Williams Racing Cars/Williams': 'Williams',\n",
    "    \n",
    "    # Renault\n",
    "    'Renault': 'Renault',\n",
    "\n",
    "    # Alpine\n",
    "    'Alpine Renault': 'Alpine',\n",
    "    \n",
    "    # Lotus\n",
    "    'Lotus Renault': 'Lotus',\n",
    "    'Lotus Ford': 'Lotus',\n",
    "    'Lotus Climax': 'Lotus',\n",
    "    'Lotus BRM': 'Lotus',\n",
    "    'Lotus Honda': 'Lotus',\n",
    "    'Lotus Judd': 'Lotus',\n",
    "    'Lotus Lamborghini': 'Lotus',\n",
    "    'Lotus Mugen Honda': 'Lotus',\n",
    "    'Lotus Mercedes': 'Lotus',\n",
    "    'Lotus Cosworth': 'Lotus',\n",
    "    'Lotus Maserati': 'Lotus',\n",
    "    'Lotus Pratt & Whitney': 'Lotus',\n",
    "    \n",
    "    # Force India\n",
    "    'Force India Ferrari': 'Force India',\n",
    "    'Force India Mercedes': 'Force India',\n",
    "\n",
    "    # Racing Point\n",
    "    'Racing Point BWT Mercedes': 'Racing Point',\n",
    "\n",
    "    # Sauber\n",
    "    'Sauber': 'Sauber',\n",
    "    'Sauber Ferrari': 'Sauber',\n",
    "    'Sauber Petronas': 'Sauber',\n",
    "    'Sauber BMW': 'Sauber',\n",
    "    'Sauber Mercedes': 'Sauber',\n",
    "    'Sauber Ford': 'Sauber',\n",
    "    'Kick Sauber Ferrari': 'Sauber',\n",
    "\n",
    "    # Alfa Romeo\n",
    "    'Alfa Romeo Racing Ferrari': 'Alfa Romeo',\n",
    "    'Alfa Romeo Ferrari': 'Alfa Romeo',\n",
    "    'Alfa Romeo': 'Alfa Romeo',\n",
    "    \n",
    "    # Haas\n",
    "    'Haas Ferrari': 'Haas',\n",
    "    'Haas F1 Team': 'Haas',\n",
    "    \n",
    "    # Jordan\n",
    "    'Jordan Ford': 'Jordan',\n",
    "    'Jordan Peugeot': 'Jordan',\n",
    "    'Jordan Hart': 'Jordan',\n",
    "    'Jordan Honda': 'Jordan',\n",
    "    'Jordan Yamaha': 'Jordan',\n",
    "    'Jordan Toyota': 'Jordan',\n",
    "    'Jordan Mugen Honda': 'Jordan',\n",
    "    \n",
    "    # BAR\n",
    "    'BAR Honda': 'BAR',\n",
    "    'BAR Supertec': 'BAR',\n",
    "    \n",
    "    # Honda\n",
    "    'Honda': 'Honda',\n",
    "    \n",
    "    # Benetton\n",
    "    'Benetton Ford': 'Benetton',\n",
    "    'Benetton BMW': 'Benetton',\n",
    "    'Benetton Renault': 'Benetton',\n",
    "    'Benetton Playlife': 'Benetton',\n",
    "    \n",
    "    # Toyota\n",
    "    'Toyota': 'Toyota',\n",
    "    \n",
    "    # Jaguar\n",
    "    'Jaguar Cosworth': 'Jaguar',\n",
    "    \n",
    "    # Stewart\n",
    "    'Stewart Ford': 'Stewart',\n",
    "    \n",
    "    # BRM\n",
    "    'BRM': 'BRM',\n",
    "    'BRM Climax': 'BRM',\n",
    "\n",
    "    # JBW\n",
    "    'JBW Maserati': 'JBW',\n",
    "    'JBW Climax': 'JBW',\n",
    "    \n",
    "    # Cooper\n",
    "    'Cooper Climax': 'Cooper',\n",
    "    'Cooper Maserati': 'Cooper',\n",
    "    'Cooper Bristol': 'Cooper',\n",
    "    'Cooper Castellotti': 'Cooper',\n",
    "    'Cooper BRM': 'Cooper',\n",
    "    'Cooper JAP': 'Cooper',\n",
    "    'Cooper Alta': 'Cooper',\n",
    "    'Cooper Borgward': 'Cooper',\n",
    "    'Cooper Alfa Romeo': 'Cooper',\n",
    "    'Cooper Ferrari': 'Cooper',\n",
    "    'Cooper ATS': 'Cooper',\n",
    "    'Cooper Ford': 'Cooper',\n",
    "    'Cooper OSCA': 'Cooper',\n",
    "    \n",
    "    # Brabham\n",
    "    'Brabham Climax': 'Brabham',\n",
    "    'Brabham Repco': 'Brabham',\n",
    "    'Brabham Ford': 'Brabham',\n",
    "    'Brabham Alfa Romeo': 'Brabham',\n",
    "    'Brabham BMW': 'Brabham',\n",
    "    'Brabham BRM': 'Brabham',\n",
    "    'Brabham Judd': 'Brabham',\n",
    "    'Brabham Yamaha': 'Brabham',\n",
    "    \n",
    "    # Maserati\n",
    "    'Maserati': 'Maserati',\n",
    "    'Maserati Offenhauser': 'Maserati',\n",
    "    'Maserati Milano': 'Maserati',\n",
    "    'Maserati-Offenhauser': 'Maserati',\n",
    "    'Maserati OSCA': 'Maserati',\n",
    "    'Maserati Plate': 'Maserati',\n",
    "    \n",
    "    # Ligier\n",
    "    'Ligier Matra': 'Ligier',\n",
    "    'Ligier Ford': 'Ligier',\n",
    "    'Ligier Renault': 'Ligier',\n",
    "    'Ligier Megatron': 'Ligier',\n",
    "    'Ligier Mugen Honda': 'Ligier',\n",
    "    \n",
    "    # Tyrrell\n",
    "    'Tyrrell Ford': 'Tyrrell',\n",
    "    'Tyrrell Renault': 'Tyrrell',\n",
    "    'Tyrrell Honda': 'Tyrrell',\n",
    "    'Tyrrell Yamaha': 'Tyrrell',\n",
    "    'Tyrrell Ilmor': 'Tyrrell',\n",
    "    \n",
    "    # Arrows/Footwork\n",
    "    'Arrows Ford': 'Arrows',\n",
    "    'Arrows BMW': 'Arrows',\n",
    "    'Arrows Megatron': 'Arrows',\n",
    "    'Arrows Yamaha': 'Arrows',\n",
    "    'Arrows Supertec': 'Arrows',\n",
    "    'Arrows Asiatech': 'Arrows',\n",
    "    'Arrows Cosworth': 'Arrows',\n",
    "    'Arrows': 'Arrows',\n",
    "    'Footwork Ford': 'Footwork',\n",
    "    'Footwork Hart': 'Footwork',\n",
    "    'Footwork Mugen Honda': 'Footwork',\n",
    "    'Footwork Porsche': 'Footwork',\n",
    "    \n",
    "    # Vanwall\n",
    "    'Vanwall': 'Vanwall',\n",
    "    \n",
    "    # Wolf\n",
    "    'Wolf Ford': 'Wolf',\n",
    "    'Wolf-Williams': 'Wolf',\n",
    "    \n",
    "    # Lola\n",
    "    'Lola Ford': 'Lola',\n",
    "    'Lola Lamborghini': 'Lola',\n",
    "    'Lola Climax': 'Lola',\n",
    "    'Lola BMW': 'Lola',\n",
    "    'Lola Hart': 'Lola',\n",
    "    'Lola Ferrari': 'Lola',\n",
    "\n",
    "    # March\n",
    "    'March Ford': 'March',\n",
    "    'March Judd': 'March',\n",
    "    'March Ilmor': 'March',\n",
    "    'March Alfa Romeo': 'March',\n",
    "\n",
    "    # Minardi\n",
    "    'Minardi Ford': 'Minardi',\n",
    "    'Minardi Ferrari': 'Minardi',\n",
    "    'Minardi Lamborghini': 'Minardi',\n",
    "    'Minardi Asiatech': 'Minardi',\n",
    "    'Minardi Cosworth': 'Minardi',\n",
    "    'Minardi Fondmetal': 'Minardi',\n",
    "    'Minardi European': 'Minardi',\n",
    "    'Minardi Hart': 'Minardi',\n",
    "    'Minardi Motori Moderni': 'Minardi',\n",
    "    \n",
    "    # LDS\n",
    "    'LDS Alfa Romeo': 'LDS',\n",
    "    'LDS Climax': 'LDS',\n",
    "    'LDS Repco': 'LDS',\n",
    "\n",
    "    # Porche\n",
    "    'Porsche (F2)': 'Porsche',\n",
    "    'Porsche': 'Porsche',\n",
    "    'Behra-Porsche': 'Porsche',\n",
    "\n",
    "    # Scirocco\n",
    "    'Scirocco BRM': 'Scirocco',\n",
    "    'Scirocco Climax': 'Scirocco',\n",
    "\n",
    "    # AFM\n",
    "    'AFM Kuchen': 'AFM',\n",
    "    'AFM BMW': 'AFM',\n",
    "    'AFM Bristol': 'AFM',\n",
    "\n",
    "    # ATS\n",
    "    'ATS Ford': 'ATS',\n",
    "    'ATS': 'ATS',\n",
    "    'ATS BMW': 'ATS',\n",
    "    'Derrington-Francis ATS': 'ATS',\n",
    "\n",
    "    # Leyton House\n",
    "    'Leyton House Judd': 'Leyton House',\n",
    "    'Leyton House Ilmor': 'Leyton House',\n",
    "\n",
    "    # Prost\n",
    "    'Prost Mugen Honda': 'Prost',\n",
    "    'Prost Peugeot': 'Prost',\n",
    "    'Prost Acer': 'Prost',\n",
    "\n",
    "    # Dallara\n",
    "    'Dallara Judd': 'Dallara',\n",
    "    'Dallara Ferrari': 'Dallara',\n",
    "    'Dallara Ford': 'Dallara',\n",
    "\n",
    "    # Larrousse\n",
    "    'Larrousse Lamborghini': 'Larrousse',\n",
    "    'Larrousse Ford': 'Larrousse',\n",
    "\n",
    "    # Osella\n",
    "    'Osella Ford': 'Osella',\n",
    "    'Osella Alfa Romeo': 'Osella',\n",
    "    'Osella': 'Osella',\n",
    "    'Osella Hart': 'Osella',\n",
    "\n",
    "    # Kurtis Kraft\n",
    "    'Kurtis Kraft Offenhauser': 'Kurtis Kraft',\n",
    "    'Kurtis Kraft Novi': 'Kurtis Kraft',\n",
    "    'Kurtis Kraft Cummins': 'Kurtis Kraft',\n",
    "\n",
    "    # Marussia\n",
    "    'Marussia Cosworth': 'Marussia',\n",
    "    'Marussia Ferrari': 'Marussia',\n",
    "\n",
    "    # Gordini\n",
    "    'Simca-Gordini': 'Gordini',\n",
    "    'Gordini': 'Gordini',\n",
    "\n",
    "    # Connaught\n",
    "    'Connaught Lea Francis': 'Connaught',\n",
    "    'Connaught Alta': 'Connaught',\n",
    "\n",
    "    # Eagle\n",
    "    'Eagle Climax': 'Eagle',\n",
    "    'Eagle Weslake': 'Eagle',\n",
    "\n",
    "    # RAM\n",
    "    'RAM Ford': 'RAM',\n",
    "    'RAM Hart': 'RAM',\n",
    "\n",
    "    # Shadow\n",
    "    'Shadow Ford': 'Shadow',\n",
    "    'Shadow Matra': 'Shadow',\n",
    "\n",
    "    # Matra\n",
    "    'Matra Ford': 'Matra',\n",
    "    'Matra': 'Matra',\n",
    "    'Matra Cosworth': 'Matra',\n",
    "    'Matra BRM': 'Matra',\n",
    "\n",
    "    # ERA\n",
    "    'ERA': 'ERA',\n",
    "    'ERA Bristol': 'ERA',\n",
    "\n",
    "    # Spirit\n",
    "    'Spirit Honda': 'Spirit',   \n",
    "    'Spirit Hart': 'Spirit',\n",
    "\n",
    "    # Frazer Nash\n",
    "    'Frazer Nash': 'Frazer Nash',\n",
    "    'Frazer Nash Bristol': 'Frazer Nash',\n",
    "\n",
    "    # Emeryson\n",
    "    'Emeryson Alta': 'Emeryson',\n",
    "    'Emeryson Climax': 'Emeryson',\n",
    "\n",
    "    # De Tomaso\n",
    "    'De Tomaso OSCA': 'De Tomaso',\n",
    "    'De Tomaso Alfa Romeo': 'De Tomaso',\n",
    "    'De Tomaso Ford': 'De Tomaso',\n",
    "\n",
    "    # Gilby\n",
    "    'Gilby Climax': 'Gilby',\n",
    "    'Gilby BRM': 'Gilby',\n",
    "\n",
    "    # Tecno\n",
    "    'Tecno': 'Tecno',\n",
    "    'Tecno Cosworth': 'Tecno',\n",
    "\n",
    "    # Ligier\n",
    "    'Ligier Judd': 'Ligier',\n",
    "    'Ligier Lamborghini': 'Ligier',\n",
    "\n",
    "    # Euro Brun\n",
    "    'Euro Brun Judd': 'Euro Brun',\n",
    "    'Euro Brun Ford': 'Euro Brun',\n",
    "\n",
    "\n",
    "    # Other\n",
    "    'No Team': 'Privateer',\n",
    "    'Toleman Hart': 'Toleman',       \n",
    "    'Venturi Lamborghini': 'Venturi',        \n",
    "    'Onyx Ford': 'Onyx',\n",
    "    'AGS Ford': 'AGS',   \n",
    "    'Rial Ford': 'Rial',\n",
    "    'Zakspeed': 'Zakspeed',\n",
    "    'Theodore Ford': 'Theodore',\n",
    "    'Deidt Offenhauser': 'Deidt',\n",
    "    'Sherman Offenhauser': 'Sherman',\n",
    "    'Schroeder Offenhauser': 'Schroeder',\n",
    "    'Kuzma Offenhauser': 'Kuzma',\n",
    "    'Lesovsky Offenhauser': 'Lesovsky',\n",
    "    'Watson Offenhauser': 'Watson',\n",
    "    'Phillips Offenhauser': 'Phillips',\n",
    "    'Epperly Offenhauser': 'Epperly',\n",
    "    'Trevis Offenhauser': 'Trevis',\n",
    "    'HRT Cosworth': 'HRT',\n",
    "    'Virgin Cosworth': 'Virgin',\n",
    "    'Caterham Renault': 'Caterham',\n",
    "    'Milano Speluzzi': 'Milano',\n",
    "    'Turner Offenhauser': 'Turner',\n",
    "    'Alta': 'Alta',    \n",
    "    'Moore Offenhauser': 'Moore',\n",
    "    'Nichels Offenhauser': 'Nichels',\n",
    "    'Marchese Offenhauser': 'Marchese',\n",
    "    'Stevens Offenhauser': 'Stevens',\n",
    "    'Langley Offenhauser': 'Langley',\n",
    "    'Ewing Offenhauser': 'Ewing',   \n",
    "    'Rae Offenhauser': 'Rae',\n",
    "    'Olson Offenhauser': 'Olson',\n",
    "    'Wetteroth Offerhauser': 'Wetteroth',\n",
    "    'Snowberger Offenhauser': 'Snowberger',\n",
    "    'Adams Offenhauser': 'Adams',\n",
    "    'HWM Alta': 'HWM',    \n",
    "    'Lancia': 'Lancia',\n",
    "    'Talbot-Lago': 'Talbot-Lago',\n",
    "    'BRP BRM': 'BRP',\n",
    "    'Hesketh Ford': 'Hesketh',\n",
    "    'Hill Ford': 'Hill',\n",
    "    'Ensign Ford': 'Ensign',\n",
    "    'Penske Ford': 'Penske',\n",
    "    'Fittipaldi Ford': 'Fittipaldi',\n",
    "    'ISO Marlboro Ford': 'ISO Marlboro',\n",
    "    'Iso Marlboro Ford': 'ISO Marlboro',\n",
    "    'Surtees Ford': 'Surtees',\n",
    "    'Parnelli Ford': 'Parnelli',\n",
    "    'Super Aguri Honda': 'Super Aguri',\n",
    "    'MRT Mercedes': 'Manor',\n",
    "    'Brawn Mercedes': 'Brawn',\n",
    "    'Spyker Ferrari': 'Spyker',\n",
    "    'MF1 Toyota': 'Midland',\n",
    "    'Veritas': 'Veritas',\n",
    "    'Pawl Offenhauser': 'Pawl',\n",
    "    'Hall Offenhauser': 'Hall',\n",
    "    'Bromme Offenhauser': 'Bromme',\n",
    "    'OSCA': 'OSCA',\n",
    "    'BMW': 'BMW',\n",
    "    'EMW': 'EMW',\n",
    "    'Pankratz Offenhauser': 'Pankratz',\n",
    "    'Bugatti': 'Bugatti',\n",
    "    'Klenk BMW': 'Klenk',\n",
    "    'Dunn Offenhauser': 'Dunn',    \n",
    "    'Elder Offenhauser': 'Elder',\n",
    "    'Christensen Offenhauser': 'Christensen',\n",
    "    'Sutton Offenhauser': 'Sutton',\n",
    "    'Tec-Mec Maserati': 'Tec-Mec',\n",
    "    'Meskowski Offenhauser': 'Meskowski',\n",
    "    'Scarab': 'Scarab',\n",
    "    'Ferguson Climax': 'Ferguson',\n",
    "    'ENB Maserati': 'ENB',\n",
    "    'Stebro Ford': 'Stebro',               \n",
    "    'Shannon Climax': 'Shannon',     \n",
    "    'Protos Cosworth': 'Protos',   \n",
    "    'Bellasi Ford': 'Bellasi',       \n",
    "    'Eifelland Ford': 'Eifelland',\n",
    "    'Politoys Ford': 'Politoys',\n",
    "    'Connew Ford': 'Connew',\n",
    "    'Trojan Ford': 'Trojan',\n",
    "    'Amon Ford': 'Amon',\n",
    "    'Token Ford': 'Token',\n",
    "    'Lyncar Ford': 'Lyncar',\n",
    "    'Boro Ford': 'Boro',\n",
    "    'Kojima Ford': 'Kojima',\n",
    "    'LEC Ford': 'LEC',\n",
    "    'Merzario Ford': 'Merzario',\n",
    "    'Martini Ford': 'Martini',\n",
    "    'Rebaque Ford': 'Rebaque',\n",
    "    'AGS Motori Moderni': 'AGS',\n",
    "    'Coloni Ford': 'Coloni',\n",
    "    'Zakspeed Yamaha': 'Zakspeed',\n",
    "    'Fondmetal Ford': 'Fondmetal',\n",
    "    'Moda Judd': 'Moda',    \n",
    "    'Simtek Ford': 'Simtek',\n",
    "    'Pacific Ilmor': 'Pacific',\n",
    "    'Forti Ford': 'Forti',\n",
    "    'Lambo Lamborghini': 'Modena'\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46cbff",
   "metadata": {},
   "source": [
    "## ID Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54edec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_id_map(path: str, default: dict | list | None = None):\n",
    "    \"\"\"\n",
    "    Load the pickle file ID maps if they exist, otherwise return an empty dictionary or list\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {} if default is None else default\n",
    "\n",
    "def is_file_locked(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a file is currently locked by another process\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r+b') as f:\n",
    "            pass\n",
    "        return False\n",
    "    except (IOError, OSError):\n",
    "        return True\n",
    "\n",
    "def save_id_map(path: str, id_map, max_retries: int = 3):\n",
    "    \"\"\"\n",
    "    Save the ID map to a pickle file with retry mechanism for permission errors\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if file is locked before attempting to save\n",
    "    if os.path.exists(path) and is_file_locked(path):\n",
    "        print(f\"Warning: {path} appears to be locked by another process\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Try to save directly first\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(id_map, f)\n",
    "            return  # Success, exit function\n",
    "            \n",
    "        except PermissionError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Permission denied on attempt {attempt + 1}, retrying in 1 second...\")\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                # Final attempt: try using a temporary file and then moving it\n",
    "                try:\n",
    "                    temp_dir = os.path.dirname(path)\n",
    "                    temp_file = tempfile.NamedTemporaryFile(mode='wb', dir=temp_dir, delete=False, suffix='.pkl')\n",
    "                    \n",
    "                    with temp_file as f:\n",
    "                        pickle.dump(id_map, f)\n",
    "                    \n",
    "                    # Move the temporary file to the target location\n",
    "                    shutil.move(temp_file.name, path)\n",
    "                    print(f\"Successfully saved {path} using temporary file method\")\n",
    "                    return\n",
    "                    \n",
    "                except Exception as final_e:\n",
    "                    print(f\"Failed to save {path} after {max_retries} attempts: {final_e}\")\n",
    "                    raise final_e\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error saving {path}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882d0b5",
   "metadata": {},
   "source": [
    "## Column-Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214682eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_col_map(col_map: dict):\n",
    "    \"\"\"\n",
    "    Takes a {column_name: column_index} dictionary as an input and returns a new dictionary with\n",
    "    indexes and empty lists\n",
    "\n",
    "    \"\"\"\n",
    "    return {col: {'index': index, 'values': []} for col, index in col_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a3735",
   "metadata": {},
   "source": [
    "## Scrape URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d4a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url_table(urls: list, total_col: int, col_idx_map: dict, id_cols: list, page_lvl_cols: list = None, data_folder: str = '../data/raw', id_mask: dict = None, auto_url_id: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes a table from a website and returns a dataframe of scraped values\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list\n",
    "        The webpage URL(s) to scrape\n",
    "    total_cols : int\n",
    "        Number of columns in the table\n",
    "    col_idx_map : dict\n",
    "        A dictionary mapping desired column names to column indices\n",
    "        Example: {'race_id': None, 'start_pos': 1, 'driver_name': 3...}\n",
    "    id_cols : list\n",
    "        List of the names of ID columns in the col_idx_map\n",
    "    page_lvl_cols : list, optional\n",
    "        List of columns that need scraping on the page level, index will\n",
    "        contain path to scrape that data\n",
    "    data_folder : str, optional\n",
    "        File path of data folder for saving any ID maps\n",
    "        Default: ../data/raw\n",
    "    id_mask : dict, optional\n",
    "        Dictionary mapping column names to value mapping dictionaries\n",
    "        Example: {'team_name': {'Red Bull Racing': 'Red Bull'}}\n",
    "    auto_url_id : bool, optional\n",
    "        Whether to automatically create URL IDs for each row\n",
    "        Default: False\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the scraped table\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initiate data mapping\n",
    "    col_data = init_col_map(col_idx_map)\n",
    "\n",
    "    # Load URL ID map only if url_id is True\n",
    "    if auto_url_id:\n",
    "        if data_folder:\n",
    "            url_id_map = load_id_map(f'{data_folder}/url_id_map.pkl')\n",
    "        else:\n",
    "            url_id_map = load_id_map('url_id_map.pkl')\n",
    "\n",
    "    # Establish web browser\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    for url in urls:\n",
    "        \n",
    "        # Validate URL\n",
    "        try:\n",
    "            browser.get(url)\n",
    "        except Exception as e:\n",
    "            print(f'URL ERROR: \"{url}\"\\n{e}')\n",
    "            continue\n",
    "\n",
    "        # Get or create URL ID only if auto_url_id is True\n",
    "        if auto_url_id:\n",
    "            if url in url_id_map:\n",
    "                url_id_val = url_id_map[url]\n",
    "            else:\n",
    "                url_id_val = max(url_id_map.values()) + 1 if url_id_map else 1\n",
    "                url_id_map[url] = url_id_val\n",
    "                # Save the updated URL ID map\n",
    "                if data_folder:\n",
    "                    save_id_map(f'{data_folder}/url_id_map.pkl', url_id_map)\n",
    "                else:\n",
    "                    save_id_map('url_id_map.pkl', url_id_map)\n",
    "\n",
    "        try:\n",
    "            # Find table data\n",
    "            table = browser.find_elements(By.TAG_NAME, 'table')\n",
    "            for tr in table:\n",
    "                rows = tr.find_elements(By.TAG_NAME, 'tr')[1:]\n",
    "                for row in rows:\n",
    "                    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                    \n",
    "                    # Validate table has the right number of columns\n",
    "                    if len(cells) == total_col:\n",
    "                        \n",
    "                        # For each column in the column map append the corresponding data\n",
    "                        for col_name, col_info in col_data.items():\n",
    "                            \n",
    "                            # Skip indexes with None\n",
    "                            if col_info['index'] == None:\n",
    "                                continue\n",
    "                            \n",
    "                            # Create IDs and ID maps\n",
    "                            if col_name in id_cols:\n",
    "                                \n",
    "                                # Load or create ID map\n",
    "                                if data_folder:\n",
    "                                    id_map = load_id_map(f'{data_folder}/{col_name}_map.pkl')\n",
    "                                else:\n",
    "                                    id_map = load_id_map(f'{col_name}_map.pkl')\n",
    "                                \n",
    "                                # Get the value from the table cell using the index from col_map\n",
    "                                if isinstance(col_info['index'], int):\n",
    "                                    scraped_value = cells[col_info['index']].text.strip()\n",
    "                                elif page_lvl_cols and col_name in page_lvl_cols:\n",
    "                                    scraped_value = col_info['index'](browser)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unsupported index type for {col_name}: {type(col_info['index'])}\")\n",
    "\n",
    "                                # Apply ID mask if provided\n",
    "                                if id_mask and col_name in id_mask:\n",
    "                                    scraped_value = id_mask[col_name].get(scraped_value, scraped_value)\n",
    "                                \n",
    "                                # Search through ID map keys to find a match\n",
    "                                matched_key = None\n",
    "                                for existing_key in id_map.keys():\n",
    "                                    if scraped_value in existing_key:\n",
    "                                        matched_key = existing_key\n",
    "                                        break\n",
    "                                \n",
    "                                # Use matched key if found, otherwise use scraped value\n",
    "                                lookup_key = matched_key if matched_key is not None else scraped_value\n",
    "                                \n",
    "                                # Append existing ID or create new key-value pair\n",
    "                                if lookup_key in id_map:\n",
    "                                    col_info['values'].append(id_map[lookup_key])\n",
    "                                else:\n",
    "                                    new_id = max(id_map.values()) + 1 if id_map else 1\n",
    "                                    id_map[lookup_key] = new_id\n",
    "                                    col_info['values'].append(new_id)\n",
    "                                \n",
    "                                # Save the updated ID map\n",
    "                                if data_folder:\n",
    "                                    save_id_map(f'{data_folder}/{col_name}_map.pkl', id_map)\n",
    "                                else:\n",
    "                                    save_id_map(f'{col_name}_map.pkl', id_map)\n",
    "                            \n",
    "                            # Handle non-ID columns\n",
    "                            else:\n",
    "                                if isinstance(col_info['index'], int):\n",
    "                                    scraped_value = cells[col_info['index']].text.strip()\n",
    "                                elif page_lvl_cols and col_name in page_lvl_cols:\n",
    "                                    scraped_value = col_info['index'](browser)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unsupported index type for {col_name}: {type(col_info['index'])}\")\n",
    "                                col_info['values'].append(scraped_value)\n",
    "                        \n",
    "                        # Append the same URL ID for every row from this URL only if auto_url_id is True\n",
    "                        if auto_url_id:\n",
    "                            if 'url_id' not in col_data:\n",
    "                                col_data['url_id'] = {'index': None, 'values': []}\n",
    "                            col_data['url_id']['values'].append(url_id_val)\n",
    "                                \n",
    "        except Exception as e:\n",
    "            print(f'NO DATA FOUND ERROR: {e}')\n",
    "    \n",
    "    browser.close()\n",
    "    \n",
    "    # Convert column data to DataFrame\n",
    "    df_data = {}\n",
    "    for col_name, col_info in col_data.items():\n",
    "        df_data[col_name] = col_info['values']\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(df_data)\n",
    "    except Exception as e:\n",
    "        print(f'ARRAY LENGTH ERROR: {e}')\n",
    "        return(f'ERROR: {e}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3b465",
   "metadata": {},
   "source": [
    "## Aggregate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ffab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_columns(df, columns: list = None, boolean_columns: list = None):\n",
    "    \"\"\"\n",
    "    Universal aggregation function that returns mean, min, max, and std values for numeric columns\n",
    "    and boolean aggregation for True/False columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    columns : list\n",
    "        List of numeric column names to aggregate. If None, aggregates all numeric columns.\n",
    "    boolean_columns : list\n",
    "        List of boolean column names to check for any True values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Aggregated statistics for the specified columns\n",
    "\n",
    "    \"\"\"\n",
    "    agg = {}\n",
    "    \n",
    "    # Handle numeric columns\n",
    "    if columns is None:\n",
    "        # Get all numeric columns if none specified\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            agg[f'{col}_mean'] = df[col].mean()\n",
    "            agg[f'{col}_min'] = df[col].min()\n",
    "            agg[f'{col}_max'] = df[col].max()\n",
    "            agg[f'{col}_std'] = df[col].std()\n",
    "    \n",
    "    # Handle boolean columns\n",
    "    if boolean_columns is not None:\n",
    "        for col in boolean_columns:\n",
    "            if col in df.columns:\n",
    "                agg[f'{col}_any'] = bool(df[col].any())\n",
    "                agg[f'{col}_mean'] = df[col].mean()\n",
    "    \n",
    "    return pd.Series(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968658e",
   "metadata": {},
   "source": [
    "# F1 Site 2001-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6714b5",
   "metadata": {},
   "source": [
    "## Race Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80ee0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish web browser and initial variables\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "year_begin = 2001\n",
    "year_end = 2017\n",
    "race_urls = []\n",
    "\n",
    "while year_begin <= year_end:\n",
    "\n",
    "    # Use the years to crawl across season pages\n",
    "    url = \"https://www.formula1.com/en/results/\" + str(year_begin) + \"/races\"\n",
    "    browser.get(url)\n",
    "    \n",
    "    table = browser.find_elements(By.TAG_NAME, \"table\")\n",
    "    for tr in table:\n",
    "        rows = tr.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "            # Url for each specific race\n",
    "            link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "            race_urls.append(link.get_attribute(\"href\"))\n",
    "    \n",
    "    year_begin += 1\n",
    "\n",
    "browser.close()\n",
    "\n",
    "# Save links to file\n",
    "load_id_map('../data/raw/links_2001_2017.pkl')\n",
    "save_id_map('../data/raw/links_2001_2017.pkl', race_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83756d04",
   "metadata": {},
   "source": [
    "## Race Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581437d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables\n",
    "urls = load_id_map('../data/raw/links_2001_2017.pkl')\n",
    "total_cols = 7\n",
    "col_idx_map = {\n",
    "    'driver_id': 2,\n",
    "    'position': 0,\n",
    "    'driver_name': 2,\n",
    "    'points': 6}\n",
    "id_cols = ['driver_id']\n",
    "\n",
    "# Scrape 2001-2017 results\n",
    "df = scrape_url_table(\n",
    "    urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols)\n",
    "df.to_csv('../data/raw/race_results_raw_2001-2017.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164dcb2",
   "metadata": {},
   "source": [
    "# F1 Site 2018+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23231d0",
   "metadata": {},
   "source": [
    "## Race Links & Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be5511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish web browser and initial variables\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "year_begin = 2018\n",
    "year_end = datetime.now().year\n",
    "race_urls = []\n",
    "round_number = []\n",
    "\n",
    "while year_begin <= year_end:\n",
    "    r = 1  \n",
    "\n",
    "    # Use the years to crawl across season pages\n",
    "    url = \"https://www.formula1.com/en/results/\" + str(year_begin) + \"/races\"\n",
    "    browser.get(url)\n",
    "    \n",
    "    table = browser.find_elements(By.TAG_NAME, \"table\")\n",
    "    for tr in table:\n",
    "        rows = tr.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "            # Url for each specific race\n",
    "            link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "            race_urls.append(link.get_attribute(\"href\"))\n",
    "            round_number.append(r)\n",
    "            r += 1 \n",
    "\n",
    "    year_begin += 1\n",
    "\n",
    "browser.close()\n",
    "\n",
    "link_data = pd.DataFrame({'race_url': race_urls, 'round_number': round_number})\n",
    "link_data.to_csv('../data/raw/rounds_raw.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Save links to file\n",
    "load_id_map('../data/raw/links_2018+.pkl')\n",
    "save_id_map('../data/raw/links_2018+.pkl', race_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9903d16",
   "metadata": {},
   "source": [
    "## Race Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "total_cols = 7\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'circuit_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text,\n",
    "    'team_id': 3,\n",
    "    'year': lambda browser: int(browser.current_url.split(\"/\")[5]),\n",
    "    'race_url': lambda browser: browser.current_url,\n",
    "    'circuit_name': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text,\n",
    "    'driver_name': 2,\n",
    "    'team_name': 3,\n",
    "    'end_position': 0,\n",
    "    'points': 6,\n",
    "    'laps_completed': 4}\n",
    "id_cols = ['race_id', 'driver_id', 'circuit_id', 'team_id']\n",
    "page_lvl_cols = ['race_id', 'circuit_id', 'year', 'race_url', 'circuit_name']\n",
    "\n",
    "# Scrape 2018+ results\n",
    "df = scrape_url_table(\n",
    "    urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/race_results_raw_2018+.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754febc2",
   "metadata": {},
   "source": [
    "## Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create practice URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "practice_urls = []\n",
    "for url in urls:\n",
    "    for practice_num in [1, 2, 3]:\n",
    "        practice_url = url.replace('/race-result', f'/practice/{practice_num}')\n",
    "        practice_urls.append(practice_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 6\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'session_type': lambda browser: browser.current_url.split(\"/\")[9] + browser.current_url.split(\"/\")[10],\n",
    "    'lap_time': 4,\n",
    "    'lap_count': 5,\n",
    "    'position': 0}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id', 'session_type']\n",
    "\n",
    "# Scrape practice results\n",
    "df = scrape_url_table(\n",
    "    practice_urls,\n",
    "    total_cols, col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/pratice_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bef32a",
   "metadata": {},
   "source": [
    "## Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create qualifying URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "qualifying_urls = []\n",
    "for url in urls:\n",
    "    qual_url = url.replace('/race-result', '/qualifying')\n",
    "    qualifying_urls.append(qual_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'q1_time': 4,\n",
    "    'q2_time': 5,\n",
    "    'q3_time': 6,\n",
    "    'qual_position': 0,\n",
    "    'qual_laps': 7}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape qualifying results\n",
    "df = scrape_url_table(\n",
    "    qualifying_urls,\n",
    "    total_cols, col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/qualifying_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c87aa8",
   "metadata": {},
   "source": [
    "## Starting Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e59620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create starting grid URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "starting_urls = []\n",
    "for url in urls:\n",
    "    start_url = url.replace('/race-result', '/starting-grid')\n",
    "    starting_urls.append(start_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 5\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'start_position': 0}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape starting grid results\n",
    "df = scrape_url_table(\n",
    "    starting_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/starting_grid_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ec1b3",
   "metadata": {},
   "source": [
    "## Pit Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b56251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pit stop URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "pit_urls = []\n",
    "for url in urls:\n",
    "    ps_url = url.replace('/race-result', '/pit-stop-summary')\n",
    "    pit_urls.append(ps_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'stop_number': 0,\n",
    "    'stop_lap': 4,\n",
    "    'pits_time': 6}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape pit stop results\n",
    "df = scrape_url_table(\n",
    "    pit_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/pit_stop_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d51364",
   "metadata": {},
   "source": [
    "## Fastest Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permission denied on attempt 1, retrying in 1 second...\n"
     ]
    }
   ],
   "source": [
    "# Create fastest lap URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "fastest_lap_urls = []\n",
    "for url in urls:\n",
    "    fastest_url = url.replace('/race-result', '/fastest-laps')\n",
    "    fastest_lap_urls.append(fastest_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'fastest_lap_time': 6,\n",
    "    'lap_number': 4}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape fastest lap results\n",
    "df = scrape_url_table(\n",
    "    fastest_lap_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/fastest_lap_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924b539",
   "metadata": {},
   "source": [
    "# FastF1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00569f46",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c523242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing race 1/167: 2018 Australia\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 2/167: 2018 Bahrain\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 3/167: 2018 China\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 4/167: 2018 Azerbaijan\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 5/167: 2018 Spain\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "Saving intermediate results after race 5...\n",
      "  Saved: 13193 laps, 2186 weather records, 1001 messages\n",
      "\n",
      "Processing race 6/167: 2018 Monaco\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 7/167: 2018 Canada\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 8/167: 2018 France\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 9/167: 2018 Austria\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 10/167: 2018 Great Britain\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "Saving intermediate results after race 10...\n",
      "  Saved: 28484 laps, 4367 weather records, 1925 messages\n",
      "\n",
      "Processing race 11/167: 2018 Germany\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 12/167: 2018 Hungary\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 13/167: 2018 Belgium\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 14/167: 2018 Italy\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackw\\AppData\\Local\\Temp\\ipykernel_24716\\4033115548.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded Qualifying\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackw\\AppData\\Local\\Temp\\ipykernel_24716\\4033115548.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded Race\n",
      "\n",
      "Processing race 15/167: 2018 Singapore\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "Saving intermediate results after race 15...\n",
      "  Saved: 42036 laps, 6709 weather records, 2666 messages\n",
      "\n",
      "Processing race 16/167: 2018 Russia\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 17/167: 2018 Japan\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackw\\AppData\\Local\\Temp\\ipykernel_24716\\4033115548.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 18/167: 2018 United States\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 19/167: 2018 Mexico\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 20/167: 2018 Brazil\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "Saving intermediate results after race 20...\n",
      "  Saved: 50537 laps, 8476 weather records, 3157 messages\n",
      "\n",
      "Processing race 21/167: 2018 Abu Dhabi\n",
      "  Loaded FP1\n",
      "  Loaded FP2\n",
      "  Loaded FP3\n",
      "  Loaded Qualifying\n",
      "  Loaded Race\n",
      "\n",
      "Processing race 22/167: 2019 Australia\n",
      "  Retry 1/5 for 2019 Australia FP1: Failed to load any schedule data.. Sleeping for 2s...\n",
      "  Retry 2/5 for 2019 Australia FP1: Failed to load any schedule data.. Sleeping for 4s...\n",
      "  Retry 3/5 for 2019 Australia FP1: Failed to load any schedule data.. Sleeping for 8s...\n",
      "  Retry 4/5 for 2019 Australia FP1: Failed to load any schedule data.. Sleeping for 16s...\n",
      "  Failed after 5 retries for 2019 Australia FP1: Failed to load any schedule data.\n",
      "  Retry 1/5 for 2019 Australia FP2: Failed to load any schedule data.. Sleeping for 2s...\n",
      "  Retry 2/5 for 2019 Australia FP2: Failed to load any schedule data.. Sleeping for 4s...\n",
      "  Retry 3/5 for 2019 Australia FP2: Failed to load any schedule data.. Sleeping for 8s...\n",
      "  Retry 4/5 for 2019 Australia FP2: Failed to load any schedule data.. Sleeping for 16s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize urls and sessions\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "sessions_collected = ['FP1', 'FP2', 'FP3', 'Qualifying', 'Race']\n",
    "fastf1.Cache.disabled = True\n",
    "\n",
    "# Suppress FastF1 logging output\n",
    "fastf1_logger = logging.getLogger('fastf1')\n",
    "fastf1_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Initialize empty DataFrames to collect all data\n",
    "all_laps = pd.DataFrame()\n",
    "all_weather = pd.DataFrame()\n",
    "all_messages = pd.DataFrame()\n",
    "\n",
    "race_id_map = load_id_map('../data/raw/race_id_map.pkl')\n",
    "\n",
    "for url_idx, url in enumerate(urls):\n",
    "    \n",
    "    # Sort year and grand prix from the url\n",
    "    year = int(url.split('/')[5])\n",
    "    gp = url.split('/')[8].replace('-', ' ').title().replace('Emilia Romagna', 'Emilia-Romagna')\n",
    "    \n",
    "    print(f\"\\nProcessing race {url_idx + 1}/{len(urls)}: {year} {gp}\")\n",
    "    \n",
    "    for s in sessions_collected:\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        laps_df = None\n",
    "        weather_df = None\n",
    "        messages_df = None\n",
    "        session = None\n",
    "\n",
    "        # Load session with retry\n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                gc.collect()\n",
    "                \n",
    "                session = fastf1.get_session(year, gp, s)\n",
    "                if retry_count > 0:\n",
    "                    time.sleep(3)\n",
    "                session.load(laps=True, telemetry=False, weather=True, messages=True)\n",
    "                \n",
    "                # Extract data with error handling\n",
    "                try:\n",
    "                    laps_df = session.laps.copy() if hasattr(session, 'laps') and session.laps is not None else None\n",
    "                except:\n",
    "                    laps_df = None\n",
    "                try:\n",
    "                    weather_df = pd.DataFrame(session.weather_data) if hasattr(session, 'weather_data') and session.weather_data is not None else None\n",
    "                except:\n",
    "                    weather_df = None\n",
    "                try:\n",
    "                    messages_df = pd.DataFrame(session.race_control_messages) if hasattr(session, 'race_control_messages') and session.race_control_messages is not None else None\n",
    "                except:\n",
    "                    messages_df = None\n",
    "                \n",
    "                success = True\n",
    "                print(f\"  Loaded {s}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    sleep_time = 2 ** retry_count\n",
    "                    print(f'  Retry {retry_count}/{max_retries} for {year} {gp} {s}: {e}. Sleeping for {sleep_time}s...')\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(f'  Failed after {max_retries} retries for {year} {gp} {s}: {e}')\n",
    "            \n",
    "            finally:\n",
    "                # Delete session object immediately to release resources\n",
    "                if session is not None:\n",
    "                    del session\n",
    "                gc.collect()\n",
    "        \n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Get race ID\n",
    "        race_key = f'{gp}_{year}'\n",
    "        race_id_value = race_id_map.get(race_key)\n",
    "        if race_id_value is None:\n",
    "            print(f'  Warning: No race_id found for: {race_key}')\n",
    "        \n",
    "        # Add race_id and session columns to each DataFrame and merge\n",
    "        if laps_df is not None and not laps_df.empty:\n",
    "            laps_df['race_id'] = race_id_value\n",
    "            laps_df['session'] = s\n",
    "            all_laps = pd.concat([all_laps, laps_df], ignore_index=True)\n",
    "        if weather_df is not None and not weather_df.empty:\n",
    "            weather_df['race_id'] = race_id_value\n",
    "            weather_df['session'] = s\n",
    "            all_weather = pd.concat([all_weather, weather_df], ignore_index=True)\n",
    "        if messages_df is not None and not messages_df.empty:\n",
    "            messages_df['race_id'] = race_id_value\n",
    "            messages_df['session'] = s\n",
    "            all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n",
    "\n",
    "        # Clean up DataFrames after each session\n",
    "        del laps_df, weather_df, messages_df\n",
    "    \n",
    "    # Force garbage collection after each race\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save intermediate results every 5 races\n",
    "    if (url_idx + 1) % 5 == 0:\n",
    "        print(f\"Saving intermediate results after race {url_idx + 1}...\")\n",
    "        all_laps.to_csv('../data/raw/lap_data_raw_temp.csv', index=False)\n",
    "        all_weather.to_csv('../data/raw/weather_data_raw_temp.csv', index=False)\n",
    "        all_messages.to_csv('../data/raw/messages_data_raw_temp.csv', index=False)\n",
    "        print(f\"  Saved: {len(all_laps)} laps, {len(all_weather)} weather records, {len(all_messages)} messages\")\n",
    "    \n",
    "# Save final DataFrames to CSVs\n",
    "all_laps.to_csv('../data/raw/lap_data_raw.csv', index=False)\n",
    "all_weather.to_csv('../data/raw/weather_data_raw.csv', index=False)\n",
    "all_messages.to_csv('../data/raw/messages_data_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c689f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.6.1]\n",
      "req            INFO \tNo cached data found for session_info. Loading data...\n",
      "_api           INFO \tFetching session info data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for driver_info. Loading data...\n",
      "_api           INFO \tFetching driver list...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for session_status_data. Loading data...\n",
      "_api           INFO \tFetching session status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for lap_count. Loading data...\n",
      "_api           INFO \tFetching lap count data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for track_status_data. Loading data...\n",
      "_api           INFO \tFetching track status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for _extended_timing_data. Loading data...\n",
      "_api           INFO \tFetching timing data...\n",
      "_api           INFO \tParsing timing data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for timing_app_data. Loading data...\n",
      "_api           INFO \tFetching timing app data...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tNo cached data found for car_data. Loading data...\n",
      "_api           INFO \tFetching car data...\n",
      "_api           INFO \tParsing car data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for position_data. Loading data...\n",
      "_api           INFO \tFetching position data...\n",
      "_api           INFO \tParsing position data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for weather_data. Loading data...\n",
      "_api           INFO \tFetching weather data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for race_control_messages. Loading data...\n",
      "_api           INFO \tFetching race control messages...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '14', '55', '44', '18', '63', '77', '10', '23', '22', '2', '20', '21', '27', '24', '4', '31', '16', '81']\n"
     ]
    }
   ],
   "source": [
    "# Enable cache (important for performance)\n",
    "# First ensure the cache directory exists\n",
    "import os\n",
    "cache_dir = \"cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    print(f\"Created cache directory: {cache_dir}\")\n",
    "\n",
    "fastf1.Cache.enable_cache(cache_dir)  # uses the created \"cache\" folder to store data\n",
    "\n",
    "# Load a session: example Bahrain GP 2023 Qualifying\n",
    "session = fastf1.get_session(2023, 'Bahrain', 'race')\n",
    "session.load(weather=True)  # only load weather data as requested\n",
    "\n",
    "# Weather data is stored in session.weather_data (a structured numpy array)\n",
    "weather_array = session.weather_data\n",
    "\n",
    "# Convert weather data to DataFrame\n",
    "weather_df = pd.DataFrame(weather_array)\n",
    "\n",
    "# Display weather data\n",
    "weather_df\n",
    "\n",
    "# Save weather dataframe to CSV file\n",
    "weather_df.to_csv(\"example_weather.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0995c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirTemp_mean        15.707865\n",
      "AirTemp_min              15.1\n",
      "AirTemp_max              16.6\n",
      "AirTemp_std           0.37574\n",
      "TrackTemp_mean      18.942135\n",
      "TrackTemp_min            18.3\n",
      "TrackTemp_max            19.4\n",
      "TrackTemp_std        0.276315\n",
      "WindSpeed_mean       3.475281\n",
      "WindSpeed_min             0.7\n",
      "WindSpeed_max             6.9\n",
      "WindSpeed_std        1.242267\n",
      "Humidity_mean       78.421348\n",
      "Humidity_min             68.0\n",
      "Humidity_max             92.0\n",
      "Humidity_std          6.50658\n",
      "Pressure_mean     1009.901685\n",
      "Pressure_min           1009.0\n",
      "Pressure_max           1010.7\n",
      "Pressure_std         0.444994\n",
      "Rainfall_any             True\n",
      "Rainfall_mean        0.325843\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example with weather data\n",
    "numeric_columns = ['AirTemp', 'TrackTemp', 'WindSpeed', 'Humidity', 'Pressure']\n",
    "boolean_columns = ['Rainfall']\n",
    "\n",
    "session_weather_features = aggregate_columns(\n",
    "    weather_df, \n",
    "    columns=numeric_columns, \n",
    "    boolean_columns=boolean_columns\n",
    ")\n",
    "\n",
    "print(session_weather_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
