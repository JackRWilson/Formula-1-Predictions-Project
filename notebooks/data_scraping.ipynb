{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8b3a9f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Jack Wilson\n",
    "9/23/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7868b9",
   "metadata": {},
   "source": [
    "This notebook outlines scraping and collecting of all data raw data used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e71531",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c8a9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, random, re, os, pickle, tempfile, shutil\n",
    "from math import e\n",
    "\n",
    "import fastf1\n",
    "import logging\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7736f",
   "metadata": {},
   "source": [
    "# DataFrame Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fc566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7e249",
   "metadata": {},
   "source": [
    "# Functions and Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a0ea2",
   "metadata": {},
   "source": [
    "## Constructor Common Name Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ac7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructors common name mapping\n",
    "constructor_mapping = {'team_id': {\n",
    "    # Red Bull\n",
    "    'Red Bull Racing Renault': 'Red Bull',\n",
    "    'Red Bull Renault': 'Red Bull',\n",
    "    'RBR Renault': 'Red Bull',\n",
    "    'RBR Cosworth': 'Red Bull',\n",
    "    'RBR Ferrari': 'Red Bull',\n",
    "    'Red Bull Racing TAG Heuer': 'Red Bull',\n",
    "    'Red Bull Racing Honda': 'Red Bull',\n",
    "    'Red Bull Racing RBPT': 'Red Bull',\n",
    "    'Red Bull Racing Honda RBPT': 'Red Bull',\n",
    "    'Red Bull Racing': 'Red Bull',\n",
    "    \n",
    "    # AlphaTauri/Toro Rosso\n",
    "    'Toro Rosso': 'Toro Rosso',\n",
    "    'STR Ferrari': 'Toro Rosso',\n",
    "    'STR Renault': 'Toro Rosso',\n",
    "    'STR Cosworth': 'Toro Rosso',\n",
    "    'Toro Rosso Ferrari': 'Toro Rosso',\n",
    "    'Scuderia Toro Rosso Honda': 'Toro Rosso',\n",
    "    'AlphaTauri Honda': 'AlphaTauri',\n",
    "    'AlphaTauri RBPT': 'AlphaTauri',\n",
    "    'AlphaTauri Honda RBPT': 'AlphaTauri',\n",
    "    \n",
    "    # Racing Bulls\n",
    "    'RB Honda RBPT': 'Racing Bulls',\n",
    "    \n",
    "    # Ferrari\n",
    "    'Ferrari': 'Ferrari',\n",
    "    'Ferrari Jaguar': 'Ferrari',\n",
    "    'Thin Wall Ferrari': 'Ferrari',\n",
    "    \n",
    "    # Mercedes\n",
    "    'Mercedes': 'Mercedes',\n",
    "    'Mercedes-Benz': 'Mercedes',\n",
    "    \n",
    "    # Aston Martin\n",
    "    'Aston Martin Mercedes': 'Aston Martin',\n",
    "    'Aston Martin Aramco Mercedes': 'Aston Martin',\n",
    "    'Aston Butterworth': 'Aston Martin',\n",
    "    'Aston Martin': 'Aston Martin',\n",
    "    \n",
    "    # McLaren\n",
    "    'McLaren Ford': 'McLaren',\n",
    "    'McLaren TAG': 'McLaren',\n",
    "    'McLaren Honda': 'McLaren',\n",
    "    'McLaren Peugeot': 'McLaren',\n",
    "    'McLaren Renault': 'McLaren',\n",
    "    'McLaren BRM': 'McLaren',\n",
    "    'McLaren Mercedes': 'McLaren',\n",
    "    'McLaren Serenissima': 'McLaren',\n",
    "    'Mclaren BRM': 'McLaren',\n",
    "    'McLaren Alfa Romeo': 'McLaren',\n",
    "    \n",
    "    # Williams\n",
    "    'Williams Ford': 'Williams',\n",
    "    'Williams Renault': 'Williams',\n",
    "    'Williams Honda': 'Williams',\n",
    "    'Williams Judd': 'Williams',\n",
    "    'Williams BMW': 'Williams',\n",
    "    'Williams Toyota': 'Williams',\n",
    "    'Williams Cosworth': 'Williams',\n",
    "    'Williams Mecachrome': 'Williams',\n",
    "    'Williams Supertec': 'Williams',\n",
    "    'Williams Mercedes': 'Williams',\n",
    "    'Frank Williams Racing Cars/Williams': 'Williams',\n",
    "    \n",
    "    # Renault\n",
    "    'Renault': 'Renault',\n",
    "\n",
    "    # Alpine\n",
    "    'Alpine Renault': 'Alpine',\n",
    "    \n",
    "    # Lotus\n",
    "    'Lotus Renault': 'Lotus',\n",
    "    'Lotus Ford': 'Lotus',\n",
    "    'Lotus Climax': 'Lotus',\n",
    "    'Lotus BRM': 'Lotus',\n",
    "    'Lotus Honda': 'Lotus',\n",
    "    'Lotus Judd': 'Lotus',\n",
    "    'Lotus Lamborghini': 'Lotus',\n",
    "    'Lotus Mugen Honda': 'Lotus',\n",
    "    'Lotus Mercedes': 'Lotus',\n",
    "    'Lotus Cosworth': 'Lotus',\n",
    "    'Lotus Maserati': 'Lotus',\n",
    "    'Lotus Pratt & Whitney': 'Lotus',\n",
    "    \n",
    "    # Force India\n",
    "    'Force India Ferrari': 'Force India',\n",
    "    'Force India Mercedes': 'Force India',\n",
    "\n",
    "    # Racing Point\n",
    "    'Racing Point BWT Mercedes': 'Racing Point',\n",
    "\n",
    "    # Sauber\n",
    "    'Sauber': 'Sauber',\n",
    "    'Sauber Ferrari': 'Sauber',\n",
    "    'Sauber Petronas': 'Sauber',\n",
    "    'Sauber BMW': 'Sauber',\n",
    "    'Sauber Mercedes': 'Sauber',\n",
    "    'Sauber Ford': 'Sauber',\n",
    "    'Kick Sauber Ferrari': 'Sauber',\n",
    "\n",
    "    # Alfa Romeo\n",
    "    'Alfa Romeo Racing Ferrari': 'Alfa Romeo',\n",
    "    'Alfa Romeo Ferrari': 'Alfa Romeo',\n",
    "    'Alfa Romeo': 'Alfa Romeo',\n",
    "    \n",
    "    # Haas\n",
    "    'Haas Ferrari': 'Haas',\n",
    "    'Haas F1 Team': 'Haas',\n",
    "    \n",
    "    # Jordan\n",
    "    'Jordan Ford': 'Jordan',\n",
    "    'Jordan Peugeot': 'Jordan',\n",
    "    'Jordan Hart': 'Jordan',\n",
    "    'Jordan Honda': 'Jordan',\n",
    "    'Jordan Yamaha': 'Jordan',\n",
    "    'Jordan Toyota': 'Jordan',\n",
    "    'Jordan Mugen Honda': 'Jordan',\n",
    "    \n",
    "    # BAR\n",
    "    'BAR Honda': 'BAR',\n",
    "    'BAR Supertec': 'BAR',\n",
    "    \n",
    "    # Honda\n",
    "    'Honda': 'Honda',\n",
    "    \n",
    "    # Benetton\n",
    "    'Benetton Ford': 'Benetton',\n",
    "    'Benetton BMW': 'Benetton',\n",
    "    'Benetton Renault': 'Benetton',\n",
    "    'Benetton Playlife': 'Benetton',\n",
    "    \n",
    "    # Toyota\n",
    "    'Toyota': 'Toyota',\n",
    "    \n",
    "    # Jaguar\n",
    "    'Jaguar Cosworth': 'Jaguar',\n",
    "    \n",
    "    # Stewart\n",
    "    'Stewart Ford': 'Stewart',\n",
    "    \n",
    "    # BRM\n",
    "    'BRM': 'BRM',\n",
    "    'BRM Climax': 'BRM',\n",
    "\n",
    "    # JBW\n",
    "    'JBW Maserati': 'JBW',\n",
    "    'JBW Climax': 'JBW',\n",
    "    \n",
    "    # Cooper\n",
    "    'Cooper Climax': 'Cooper',\n",
    "    'Cooper Maserati': 'Cooper',\n",
    "    'Cooper Bristol': 'Cooper',\n",
    "    'Cooper Castellotti': 'Cooper',\n",
    "    'Cooper BRM': 'Cooper',\n",
    "    'Cooper JAP': 'Cooper',\n",
    "    'Cooper Alta': 'Cooper',\n",
    "    'Cooper Borgward': 'Cooper',\n",
    "    'Cooper Alfa Romeo': 'Cooper',\n",
    "    'Cooper Ferrari': 'Cooper',\n",
    "    'Cooper ATS': 'Cooper',\n",
    "    'Cooper Ford': 'Cooper',\n",
    "    'Cooper OSCA': 'Cooper',\n",
    "    \n",
    "    # Brabham\n",
    "    'Brabham Climax': 'Brabham',\n",
    "    'Brabham Repco': 'Brabham',\n",
    "    'Brabham Ford': 'Brabham',\n",
    "    'Brabham Alfa Romeo': 'Brabham',\n",
    "    'Brabham BMW': 'Brabham',\n",
    "    'Brabham BRM': 'Brabham',\n",
    "    'Brabham Judd': 'Brabham',\n",
    "    'Brabham Yamaha': 'Brabham',\n",
    "    \n",
    "    # Maserati\n",
    "    'Maserati': 'Maserati',\n",
    "    'Maserati Offenhauser': 'Maserati',\n",
    "    'Maserati Milano': 'Maserati',\n",
    "    'Maserati-Offenhauser': 'Maserati',\n",
    "    'Maserati OSCA': 'Maserati',\n",
    "    'Maserati Plate': 'Maserati',\n",
    "    \n",
    "    # Ligier\n",
    "    'Ligier Matra': 'Ligier',\n",
    "    'Ligier Ford': 'Ligier',\n",
    "    'Ligier Renault': 'Ligier',\n",
    "    'Ligier Megatron': 'Ligier',\n",
    "    'Ligier Mugen Honda': 'Ligier',\n",
    "    \n",
    "    # Tyrrell\n",
    "    'Tyrrell Ford': 'Tyrrell',\n",
    "    'Tyrrell Renault': 'Tyrrell',\n",
    "    'Tyrrell Honda': 'Tyrrell',\n",
    "    'Tyrrell Yamaha': 'Tyrrell',\n",
    "    'Tyrrell Ilmor': 'Tyrrell',\n",
    "    \n",
    "    # Arrows/Footwork\n",
    "    'Arrows Ford': 'Arrows',\n",
    "    'Arrows BMW': 'Arrows',\n",
    "    'Arrows Megatron': 'Arrows',\n",
    "    'Arrows Yamaha': 'Arrows',\n",
    "    'Arrows Supertec': 'Arrows',\n",
    "    'Arrows Asiatech': 'Arrows',\n",
    "    'Arrows Cosworth': 'Arrows',\n",
    "    'Arrows': 'Arrows',\n",
    "    'Footwork Ford': 'Footwork',\n",
    "    'Footwork Hart': 'Footwork',\n",
    "    'Footwork Mugen Honda': 'Footwork',\n",
    "    'Footwork Porsche': 'Footwork',\n",
    "    \n",
    "    # Vanwall\n",
    "    'Vanwall': 'Vanwall',\n",
    "    \n",
    "    # Wolf\n",
    "    'Wolf Ford': 'Wolf',\n",
    "    'Wolf-Williams': 'Wolf',\n",
    "    \n",
    "    # Lola\n",
    "    'Lola Ford': 'Lola',\n",
    "    'Lola Lamborghini': 'Lola',\n",
    "    'Lola Climax': 'Lola',\n",
    "    'Lola BMW': 'Lola',\n",
    "    'Lola Hart': 'Lola',\n",
    "    'Lola Ferrari': 'Lola',\n",
    "\n",
    "    # March\n",
    "    'March Ford': 'March',\n",
    "    'March Judd': 'March',\n",
    "    'March Ilmor': 'March',\n",
    "    'March Alfa Romeo': 'March',\n",
    "\n",
    "    # Minardi\n",
    "    'Minardi Ford': 'Minardi',\n",
    "    'Minardi Ferrari': 'Minardi',\n",
    "    'Minardi Lamborghini': 'Minardi',\n",
    "    'Minardi Asiatech': 'Minardi',\n",
    "    'Minardi Cosworth': 'Minardi',\n",
    "    'Minardi Fondmetal': 'Minardi',\n",
    "    'Minardi European': 'Minardi',\n",
    "    'Minardi Hart': 'Minardi',\n",
    "    'Minardi Motori Moderni': 'Minardi',\n",
    "    \n",
    "    # LDS\n",
    "    'LDS Alfa Romeo': 'LDS',\n",
    "    'LDS Climax': 'LDS',\n",
    "    'LDS Repco': 'LDS',\n",
    "\n",
    "    # Porche\n",
    "    'Porsche (F2)': 'Porsche',\n",
    "    'Porsche': 'Porsche',\n",
    "    'Behra-Porsche': 'Porsche',\n",
    "\n",
    "    # Scirocco\n",
    "    'Scirocco BRM': 'Scirocco',\n",
    "    'Scirocco Climax': 'Scirocco',\n",
    "\n",
    "    # AFM\n",
    "    'AFM Kuchen': 'AFM',\n",
    "    'AFM BMW': 'AFM',\n",
    "    'AFM Bristol': 'AFM',\n",
    "\n",
    "    # ATS\n",
    "    'ATS Ford': 'ATS',\n",
    "    'ATS': 'ATS',\n",
    "    'ATS BMW': 'ATS',\n",
    "    'Derrington-Francis ATS': 'ATS',\n",
    "\n",
    "    # Leyton House\n",
    "    'Leyton House Judd': 'Leyton House',\n",
    "    'Leyton House Ilmor': 'Leyton House',\n",
    "\n",
    "    # Prost\n",
    "    'Prost Mugen Honda': 'Prost',\n",
    "    'Prost Peugeot': 'Prost',\n",
    "    'Prost Acer': 'Prost',\n",
    "\n",
    "    # Dallara\n",
    "    'Dallara Judd': 'Dallara',\n",
    "    'Dallara Ferrari': 'Dallara',\n",
    "    'Dallara Ford': 'Dallara',\n",
    "\n",
    "    # Larrousse\n",
    "    'Larrousse Lamborghini': 'Larrousse',\n",
    "    'Larrousse Ford': 'Larrousse',\n",
    "\n",
    "    # Osella\n",
    "    'Osella Ford': 'Osella',\n",
    "    'Osella Alfa Romeo': 'Osella',\n",
    "    'Osella': 'Osella',\n",
    "    'Osella Hart': 'Osella',\n",
    "\n",
    "    # Kurtis Kraft\n",
    "    'Kurtis Kraft Offenhauser': 'Kurtis Kraft',\n",
    "    'Kurtis Kraft Novi': 'Kurtis Kraft',\n",
    "    'Kurtis Kraft Cummins': 'Kurtis Kraft',\n",
    "\n",
    "    # Marussia\n",
    "    'Marussia Cosworth': 'Marussia',\n",
    "    'Marussia Ferrari': 'Marussia',\n",
    "\n",
    "    # Gordini\n",
    "    'Simca-Gordini': 'Gordini',\n",
    "    'Gordini': 'Gordini',\n",
    "\n",
    "    # Connaught\n",
    "    'Connaught Lea Francis': 'Connaught',\n",
    "    'Connaught Alta': 'Connaught',\n",
    "\n",
    "    # Eagle\n",
    "    'Eagle Climax': 'Eagle',\n",
    "    'Eagle Weslake': 'Eagle',\n",
    "\n",
    "    # RAM\n",
    "    'RAM Ford': 'RAM',\n",
    "    'RAM Hart': 'RAM',\n",
    "\n",
    "    # Shadow\n",
    "    'Shadow Ford': 'Shadow',\n",
    "    'Shadow Matra': 'Shadow',\n",
    "\n",
    "    # Matra\n",
    "    'Matra Ford': 'Matra',\n",
    "    'Matra': 'Matra',\n",
    "    'Matra Cosworth': 'Matra',\n",
    "    'Matra BRM': 'Matra',\n",
    "\n",
    "    # ERA\n",
    "    'ERA': 'ERA',\n",
    "    'ERA Bristol': 'ERA',\n",
    "\n",
    "    # Spirit\n",
    "    'Spirit Honda': 'Spirit',   \n",
    "    'Spirit Hart': 'Spirit',\n",
    "\n",
    "    # Frazer Nash\n",
    "    'Frazer Nash': 'Frazer Nash',\n",
    "    'Frazer Nash Bristol': 'Frazer Nash',\n",
    "\n",
    "    # Emeryson\n",
    "    'Emeryson Alta': 'Emeryson',\n",
    "    'Emeryson Climax': 'Emeryson',\n",
    "\n",
    "    # De Tomaso\n",
    "    'De Tomaso OSCA': 'De Tomaso',\n",
    "    'De Tomaso Alfa Romeo': 'De Tomaso',\n",
    "    'De Tomaso Ford': 'De Tomaso',\n",
    "\n",
    "    # Gilby\n",
    "    'Gilby Climax': 'Gilby',\n",
    "    'Gilby BRM': 'Gilby',\n",
    "\n",
    "    # Tecno\n",
    "    'Tecno': 'Tecno',\n",
    "    'Tecno Cosworth': 'Tecno',\n",
    "\n",
    "    # Ligier\n",
    "    'Ligier Judd': 'Ligier',\n",
    "    'Ligier Lamborghini': 'Ligier',\n",
    "\n",
    "    # Euro Brun\n",
    "    'Euro Brun Judd': 'Euro Brun',\n",
    "    'Euro Brun Ford': 'Euro Brun',\n",
    "\n",
    "\n",
    "    # Other\n",
    "    'No Team': 'Privateer',\n",
    "    'Toleman Hart': 'Toleman',       \n",
    "    'Venturi Lamborghini': 'Venturi',        \n",
    "    'Onyx Ford': 'Onyx',\n",
    "    'AGS Ford': 'AGS',   \n",
    "    'Rial Ford': 'Rial',\n",
    "    'Zakspeed': 'Zakspeed',\n",
    "    'Theodore Ford': 'Theodore',\n",
    "    'Deidt Offenhauser': 'Deidt',\n",
    "    'Sherman Offenhauser': 'Sherman',\n",
    "    'Schroeder Offenhauser': 'Schroeder',\n",
    "    'Kuzma Offenhauser': 'Kuzma',\n",
    "    'Lesovsky Offenhauser': 'Lesovsky',\n",
    "    'Watson Offenhauser': 'Watson',\n",
    "    'Phillips Offenhauser': 'Phillips',\n",
    "    'Epperly Offenhauser': 'Epperly',\n",
    "    'Trevis Offenhauser': 'Trevis',\n",
    "    'HRT Cosworth': 'HRT',\n",
    "    'Virgin Cosworth': 'Virgin',\n",
    "    'Caterham Renault': 'Caterham',\n",
    "    'Milano Speluzzi': 'Milano',\n",
    "    'Turner Offenhauser': 'Turner',\n",
    "    'Alta': 'Alta',    \n",
    "    'Moore Offenhauser': 'Moore',\n",
    "    'Nichels Offenhauser': 'Nichels',\n",
    "    'Marchese Offenhauser': 'Marchese',\n",
    "    'Stevens Offenhauser': 'Stevens',\n",
    "    'Langley Offenhauser': 'Langley',\n",
    "    'Ewing Offenhauser': 'Ewing',   \n",
    "    'Rae Offenhauser': 'Rae',\n",
    "    'Olson Offenhauser': 'Olson',\n",
    "    'Wetteroth Offerhauser': 'Wetteroth',\n",
    "    'Snowberger Offenhauser': 'Snowberger',\n",
    "    'Adams Offenhauser': 'Adams',\n",
    "    'HWM Alta': 'HWM',    \n",
    "    'Lancia': 'Lancia',\n",
    "    'Talbot-Lago': 'Talbot-Lago',\n",
    "    'BRP BRM': 'BRP',\n",
    "    'Hesketh Ford': 'Hesketh',\n",
    "    'Hill Ford': 'Hill',\n",
    "    'Ensign Ford': 'Ensign',\n",
    "    'Penske Ford': 'Penske',\n",
    "    'Fittipaldi Ford': 'Fittipaldi',\n",
    "    'ISO Marlboro Ford': 'ISO Marlboro',\n",
    "    'Iso Marlboro Ford': 'ISO Marlboro',\n",
    "    'Surtees Ford': 'Surtees',\n",
    "    'Parnelli Ford': 'Parnelli',\n",
    "    'Super Aguri Honda': 'Super Aguri',\n",
    "    'MRT Mercedes': 'Manor',\n",
    "    'Brawn Mercedes': 'Brawn',\n",
    "    'Spyker Ferrari': 'Spyker',\n",
    "    'MF1 Toyota': 'Midland',\n",
    "    'Veritas': 'Veritas',\n",
    "    'Pawl Offenhauser': 'Pawl',\n",
    "    'Hall Offenhauser': 'Hall',\n",
    "    'Bromme Offenhauser': 'Bromme',\n",
    "    'OSCA': 'OSCA',\n",
    "    'BMW': 'BMW',\n",
    "    'EMW': 'EMW',\n",
    "    'Pankratz Offenhauser': 'Pankratz',\n",
    "    'Bugatti': 'Bugatti',\n",
    "    'Klenk BMW': 'Klenk',\n",
    "    'Dunn Offenhauser': 'Dunn',    \n",
    "    'Elder Offenhauser': 'Elder',\n",
    "    'Christensen Offenhauser': 'Christensen',\n",
    "    'Sutton Offenhauser': 'Sutton',\n",
    "    'Tec-Mec Maserati': 'Tec-Mec',\n",
    "    'Meskowski Offenhauser': 'Meskowski',\n",
    "    'Scarab': 'Scarab',\n",
    "    'Ferguson Climax': 'Ferguson',\n",
    "    'ENB Maserati': 'ENB',\n",
    "    'Stebro Ford': 'Stebro',               \n",
    "    'Shannon Climax': 'Shannon',     \n",
    "    'Protos Cosworth': 'Protos',   \n",
    "    'Bellasi Ford': 'Bellasi',       \n",
    "    'Eifelland Ford': 'Eifelland',\n",
    "    'Politoys Ford': 'Politoys',\n",
    "    'Connew Ford': 'Connew',\n",
    "    'Trojan Ford': 'Trojan',\n",
    "    'Amon Ford': 'Amon',\n",
    "    'Token Ford': 'Token',\n",
    "    'Lyncar Ford': 'Lyncar',\n",
    "    'Boro Ford': 'Boro',\n",
    "    'Kojima Ford': 'Kojima',\n",
    "    'LEC Ford': 'LEC',\n",
    "    'Merzario Ford': 'Merzario',\n",
    "    'Martini Ford': 'Martini',\n",
    "    'Rebaque Ford': 'Rebaque',\n",
    "    'AGS Motori Moderni': 'AGS',\n",
    "    'Coloni Ford': 'Coloni',\n",
    "    'Zakspeed Yamaha': 'Zakspeed',\n",
    "    'Fondmetal Ford': 'Fondmetal',\n",
    "    'Moda Judd': 'Moda',    \n",
    "    'Simtek Ford': 'Simtek',\n",
    "    'Pacific Ilmor': 'Pacific',\n",
    "    'Forti Ford': 'Forti',\n",
    "    'Lambo Lamborghini': 'Modena'\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46cbff",
   "metadata": {},
   "source": [
    "## ID Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54edec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_id_map(path: str, default: dict | list | None = None):\n",
    "    \"\"\"\n",
    "    Load the pickle file ID maps if they exist, otherwise return an empty dictionary or list\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {} if default is None else default\n",
    "\n",
    "def is_file_locked(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a file is currently locked by another process\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r+b') as f:\n",
    "            pass\n",
    "        return False\n",
    "    except (IOError, OSError):\n",
    "        return True\n",
    "\n",
    "def save_id_map(path: str, id_map, max_retries: int = 3):\n",
    "    \"\"\"\n",
    "    Save the ID map to a pickle file with retry mechanism for permission errors\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if file is locked before attempting to save\n",
    "    if os.path.exists(path) and is_file_locked(path):\n",
    "        print(f\"Warning: {path} appears to be locked by another process\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Try to save directly first\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(id_map, f)\n",
    "            return  # Success, exit function\n",
    "            \n",
    "        except PermissionError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Permission denied on attempt {attempt + 1}, retrying in 1 second...\")\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                # Final attempt: try using a temporary file and then moving it\n",
    "                try:\n",
    "                    temp_dir = os.path.dirname(path)\n",
    "                    temp_file = tempfile.NamedTemporaryFile(mode='wb', dir=temp_dir, delete=False, suffix='.pkl')\n",
    "                    \n",
    "                    with temp_file as f:\n",
    "                        pickle.dump(id_map, f)\n",
    "                    \n",
    "                    # Move the temporary file to the target location\n",
    "                    shutil.move(temp_file.name, path)\n",
    "                    print(f\"Successfully saved {path} using temporary file method\")\n",
    "                    return\n",
    "                    \n",
    "                except Exception as final_e:\n",
    "                    print(f\"Failed to save {path} after {max_retries} attempts: {final_e}\")\n",
    "                    raise final_e\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error saving {path}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882d0b5",
   "metadata": {},
   "source": [
    "## Column-Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214682eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_col_map(col_map: dict):\n",
    "    \"\"\"\n",
    "    Takes a {column_name: column_index} dictionary as an input and returns a new dictionary with\n",
    "    indexes and empty lists\n",
    "\n",
    "    \"\"\"\n",
    "    return {col: {'index': index, 'values': []} for col, index in col_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a3735",
   "metadata": {},
   "source": [
    "## Scrape URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d4a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url_table(urls: list, total_col: int, col_idx_map: dict, id_cols: list, page_lvl_cols: list = None, data_folder: str = '../data/raw', id_mask: dict = None, auto_url_id: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes a table from a website and returns a dataframe of scraped values\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list\n",
    "        The webpage URL(s) to scrape\n",
    "    total_cols : int\n",
    "        Number of columns in the table\n",
    "    col_idx_map : dict\n",
    "        A dictionary mapping desired column names to column indices\n",
    "        Example: {'race_id': None, 'start_pos': 1, 'driver_name': 3...}\n",
    "    id_cols : list\n",
    "        List of the names of ID columns in the col_idx_map\n",
    "    page_lvl_cols : list, optional\n",
    "        List of columns that need scraping on the page level, index will\n",
    "        contain path to scrape that data\n",
    "    data_folder : str, optional\n",
    "        File path of data folder for saving any ID maps\n",
    "        Default: ../data/raw\n",
    "    id_mask : dict, optional\n",
    "        Dictionary mapping column names to value mapping dictionaries\n",
    "        Example: {'team_name': {'Red Bull Racing': 'Red Bull'}}\n",
    "    auto_url_id : bool, optional\n",
    "        Whether to automatically create URL IDs for each row\n",
    "        Default: False\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the scraped table\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initiate data mapping\n",
    "    col_data = init_col_map(col_idx_map)\n",
    "\n",
    "    # Load URL ID map only if url_id is True\n",
    "    if auto_url_id:\n",
    "        if data_folder:\n",
    "            url_id_map = load_id_map(f'{data_folder}/url_id_map.pkl')\n",
    "        else:\n",
    "            url_id_map = load_id_map('url_id_map.pkl')\n",
    "\n",
    "    # Establish web browser\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    for url in urls:\n",
    "        \n",
    "        # Validate URL\n",
    "        try:\n",
    "            browser.get(url)\n",
    "        except Exception as e:\n",
    "            print(f'URL ERROR: \"{url}\"\\n{e}')\n",
    "            continue\n",
    "\n",
    "        # Get or create URL ID only if auto_url_id is True\n",
    "        if auto_url_id:\n",
    "            if url in url_id_map:\n",
    "                url_id_val = url_id_map[url]\n",
    "            else:\n",
    "                url_id_val = max(url_id_map.values()) + 1 if url_id_map else 1\n",
    "                url_id_map[url] = url_id_val\n",
    "                # Save the updated URL ID map\n",
    "                if data_folder:\n",
    "                    save_id_map(f'{data_folder}/url_id_map.pkl', url_id_map)\n",
    "                else:\n",
    "                    save_id_map('url_id_map.pkl', url_id_map)\n",
    "\n",
    "        try:\n",
    "            # Find table data\n",
    "            table = browser.find_elements(By.TAG_NAME, 'table')\n",
    "            for tr in table:\n",
    "                rows = tr.find_elements(By.TAG_NAME, 'tr')[1:]\n",
    "                for row in rows:\n",
    "                    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                    \n",
    "                    # Validate table has the right number of columns\n",
    "                    if len(cells) == total_col:\n",
    "                        \n",
    "                        # For each column in the column map append the corresponding data\n",
    "                        for col_name, col_info in col_data.items():\n",
    "                            \n",
    "                            # Skip indexes with None\n",
    "                            if col_info['index'] == None:\n",
    "                                continue\n",
    "                            \n",
    "                            # Create IDs and ID maps\n",
    "                            if col_name in id_cols:\n",
    "                                \n",
    "                                # Load or create ID map\n",
    "                                if data_folder:\n",
    "                                    id_map = load_id_map(f'{data_folder}/{col_name}_map.pkl')\n",
    "                                else:\n",
    "                                    id_map = load_id_map(f'{col_name}_map.pkl')\n",
    "                                \n",
    "                                # Get the value from the table cell using the index from col_map\n",
    "                                if isinstance(col_info['index'], int):\n",
    "                                    scraped_value = cells[col_info['index']].text.strip()\n",
    "                                elif page_lvl_cols and col_name in page_lvl_cols:\n",
    "                                    scraped_value = col_info['index'](browser)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unsupported index type for {col_name}: {type(col_info['index'])}\")\n",
    "\n",
    "                                # Apply ID mask if provided\n",
    "                                if id_mask and col_name in id_mask:\n",
    "                                    scraped_value = id_mask[col_name].get(scraped_value, scraped_value)\n",
    "                                \n",
    "                                # Search through ID map keys to find a match\n",
    "                                matched_key = None\n",
    "                                for existing_key in id_map.keys():\n",
    "                                    if scraped_value in existing_key:\n",
    "                                        matched_key = existing_key\n",
    "                                        break\n",
    "                                \n",
    "                                # Use matched key if found, otherwise use scraped value\n",
    "                                lookup_key = matched_key if matched_key is not None else scraped_value\n",
    "                                \n",
    "                                # Append existing ID or create new key-value pair\n",
    "                                if lookup_key in id_map:\n",
    "                                    col_info['values'].append(id_map[lookup_key])\n",
    "                                else:\n",
    "                                    new_id = max(id_map.values()) + 1 if id_map else 1\n",
    "                                    id_map[lookup_key] = new_id\n",
    "                                    col_info['values'].append(new_id)\n",
    "                                \n",
    "                                # Save the updated ID map\n",
    "                                if data_folder:\n",
    "                                    save_id_map(f'{data_folder}/{col_name}_map.pkl', id_map)\n",
    "                                else:\n",
    "                                    save_id_map(f'{col_name}_map.pkl', id_map)\n",
    "                            \n",
    "                            # Handle non-ID columns\n",
    "                            else:\n",
    "                                if isinstance(col_info['index'], int):\n",
    "                                    scraped_value = cells[col_info['index']].text.strip()\n",
    "                                elif page_lvl_cols and col_name in page_lvl_cols:\n",
    "                                    scraped_value = col_info['index'](browser)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unsupported index type for {col_name}: {type(col_info['index'])}\")\n",
    "                                col_info['values'].append(scraped_value)\n",
    "                        \n",
    "                        # Append the same URL ID for every row from this URL only if auto_url_id is True\n",
    "                        if auto_url_id:\n",
    "                            if 'url_id' not in col_data:\n",
    "                                col_data['url_id'] = {'index': None, 'values': []}\n",
    "                            col_data['url_id']['values'].append(url_id_val)\n",
    "                                \n",
    "        except Exception as e:\n",
    "            print(f'NO DATA FOUND ERROR: {e}')\n",
    "    \n",
    "    browser.close()\n",
    "    \n",
    "    # Convert column data to DataFrame\n",
    "    df_data = {}\n",
    "    for col_name, col_info in col_data.items():\n",
    "        df_data[col_name] = col_info['values']\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(df_data)\n",
    "    except Exception as e:\n",
    "        print(f'ARRAY LENGTH ERROR: {e}')\n",
    "        return(f'ERROR: {e}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3b465",
   "metadata": {},
   "source": [
    "## Aggregate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ffab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_columns(df, columns: list = None, boolean_columns: list = None):\n",
    "    \"\"\"\n",
    "    Universal aggregation function that returns mean, min, max, and std values for numeric columns\n",
    "    and boolean aggregation for True/False columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    columns : list\n",
    "        List of numeric column names to aggregate. If None, aggregates all numeric columns.\n",
    "    boolean_columns : list\n",
    "        List of boolean column names to check for any True values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Aggregated statistics for the specified columns\n",
    "\n",
    "    \"\"\"\n",
    "    agg = {}\n",
    "    \n",
    "    # Handle numeric columns\n",
    "    if columns is None:\n",
    "        # Get all numeric columns if none specified\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            agg[f'{col}_mean'] = df[col].mean()\n",
    "            agg[f'{col}_min'] = df[col].min()\n",
    "            agg[f'{col}_max'] = df[col].max()\n",
    "            agg[f'{col}_std'] = df[col].std()\n",
    "    \n",
    "    # Handle boolean columns\n",
    "    if boolean_columns is not None:\n",
    "        for col in boolean_columns:\n",
    "            if col in df.columns:\n",
    "                agg[f'{col}_any'] = bool(df[col].any())\n",
    "                agg[f'{col}_mean'] = df[col].mean()\n",
    "    \n",
    "    return pd.Series(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968658e",
   "metadata": {},
   "source": [
    "# F1 Site 2001-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6714b5",
   "metadata": {},
   "source": [
    "## Race Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80ee0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish web browser and initial variables\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "year_begin = 2001\n",
    "year_end = 2017\n",
    "race_urls = []\n",
    "\n",
    "while year_begin <= year_end:\n",
    "\n",
    "    # Use the years to crawl across season pages\n",
    "    url = \"https://www.formula1.com/en/results/\" + str(year_begin) + \"/races\"\n",
    "    browser.get(url)\n",
    "    \n",
    "    table = browser.find_elements(By.TAG_NAME, \"table\")\n",
    "    for tr in table:\n",
    "        rows = tr.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "            # Url for each specific race\n",
    "            link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "            race_urls.append(link.get_attribute(\"href\"))\n",
    "    \n",
    "    year_begin += 1\n",
    "\n",
    "browser.close()\n",
    "\n",
    "# Save links to file\n",
    "load_id_map('../data/raw/links_2001_2017.pkl')\n",
    "save_id_map('../data/raw/links_2001_2017.pkl', race_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83756d04",
   "metadata": {},
   "source": [
    "## Race Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581437d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables\n",
    "urls = load_id_map('../data/raw/links_2001_2017.pkl')\n",
    "total_cols = 7\n",
    "col_idx_map = {\n",
    "    'driver_id': 2,\n",
    "    'position': 0,\n",
    "    'driver_name': 2,\n",
    "    'points': 6}\n",
    "id_cols = ['driver_id']\n",
    "\n",
    "# Scrape 2001-2017 results\n",
    "df = scrape_url_table(\n",
    "    urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols)\n",
    "df.to_csv('../data/raw/race_results_raw_2001-2017.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164dcb2",
   "metadata": {},
   "source": [
    "# F1 Site 2018+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23231d0",
   "metadata": {},
   "source": [
    "## Race Links & Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be5511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish web browser and initial variables\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "year_begin = 2018\n",
    "year_end = datetime.now().year\n",
    "race_urls = []\n",
    "round_number = []\n",
    "\n",
    "while year_begin <= year_end:\n",
    "    r = 1  \n",
    "\n",
    "    # Use the years to crawl across season pages\n",
    "    url = \"https://www.formula1.com/en/results/\" + str(year_begin) + \"/races\"\n",
    "    browser.get(url)\n",
    "    \n",
    "    table = browser.find_elements(By.TAG_NAME, \"table\")\n",
    "    for tr in table:\n",
    "        rows = tr.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "            # Url for each specific race\n",
    "            link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "            race_urls.append(link.get_attribute(\"href\"))\n",
    "            round_number.append(r)\n",
    "            r += 1 \n",
    "\n",
    "    year_begin += 1\n",
    "\n",
    "browser.close()\n",
    "\n",
    "link_data = pd.DataFrame({'race_url': race_urls, 'round_number': round_number})\n",
    "link_data.to_csv('../data/raw/rounds_raw.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Save links to file\n",
    "load_id_map('../data/raw/links_2018+.pkl')\n",
    "save_id_map('../data/raw/links_2018+.pkl', race_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9903d16",
   "metadata": {},
   "source": [
    "## Race Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "total_cols = 7\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'circuit_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text,\n",
    "    'team_id': 3,\n",
    "    'year': lambda browser: int(browser.current_url.split(\"/\")[5]),\n",
    "    'race_url': lambda browser: browser.current_url,\n",
    "    'circuit_name': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text,\n",
    "    'driver_name': 2,\n",
    "    'team_name': 3,\n",
    "    'end_position': 0,\n",
    "    'points': 6,\n",
    "    'laps_completed': 4}\n",
    "id_cols = ['race_id', 'driver_id', 'circuit_id', 'team_id']\n",
    "page_lvl_cols = ['race_id', 'circuit_id', 'year', 'race_url', 'circuit_name']\n",
    "\n",
    "# Scrape 2018+ results\n",
    "df = scrape_url_table(\n",
    "    urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/race_results_raw_2018+.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754febc2",
   "metadata": {},
   "source": [
    "## Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create practice URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "practice_urls = []\n",
    "for url in urls:\n",
    "    for practice_num in [1, 2, 3]:\n",
    "        practice_url = url.replace('/race-result', f'/practice/{practice_num}')\n",
    "        practice_urls.append(practice_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 6\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'session_type': lambda browser: browser.current_url.split(\"/\")[9] + browser.current_url.split(\"/\")[10],\n",
    "    'lap_time': 4,\n",
    "    'lap_count': 5,\n",
    "    'position': 0}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id', 'session_type']\n",
    "\n",
    "# Scrape practice results\n",
    "df = scrape_url_table(\n",
    "    practice_urls,\n",
    "    total_cols, col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/pratice_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bef32a",
   "metadata": {},
   "source": [
    "## Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create qualifying URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "qualifying_urls = []\n",
    "for url in urls:\n",
    "    qual_url = url.replace('/race-result', '/qualifying')\n",
    "    qualifying_urls.append(qual_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'q1_time': 4,\n",
    "    'q2_time': 5,\n",
    "    'q3_time': 6,\n",
    "    'qual_position': 0,\n",
    "    'qual_laps': 7}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape qualifying results\n",
    "df = scrape_url_table(\n",
    "    qualifying_urls,\n",
    "    total_cols, col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/qualifying_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c87aa8",
   "metadata": {},
   "source": [
    "## Starting Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e59620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create starting grid URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "starting_urls = []\n",
    "for url in urls:\n",
    "    start_url = url.replace('/race-result', '/starting-grid')\n",
    "    starting_urls.append(start_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 5\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'start_position': 0}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape starting grid results\n",
    "df = scrape_url_table(\n",
    "    starting_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/starting_grid_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ec1b3",
   "metadata": {},
   "source": [
    "## Pit Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b56251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pit stop URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "pit_urls = []\n",
    "for url in urls:\n",
    "    ps_url = url.replace('/race-result', '/pit-stop-summary')\n",
    "    pit_urls.append(ps_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'stop_number': 0,\n",
    "    'stop_lap': 4,\n",
    "    'pits_time': 6}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape pit stop results\n",
    "df = scrape_url_table(\n",
    "    pit_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/pit_stop_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d51364",
   "metadata": {},
   "source": [
    "## Fastest Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permission denied on attempt 1, retrying in 1 second...\n"
     ]
    }
   ],
   "source": [
    "# Create fastest lap URLs\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "fastest_lap_urls = []\n",
    "for url in urls:\n",
    "    fastest_url = url.replace('/race-result', '/fastest-laps')\n",
    "    fastest_lap_urls.append(fastest_url)\n",
    "\n",
    "# Establish other variables\n",
    "total_cols = 8\n",
    "col_idx_map = {\n",
    "    'race_id': lambda browser: browser.find_element(By.ID, \"content-dropdown\").text + '_' + browser.current_url.split(\"/\")[5],\n",
    "    'driver_id': 2,\n",
    "    'team_id': 3,\n",
    "    'fastest_lap_time': 6,\n",
    "    'lap_number': 4}\n",
    "id_cols = ['race_id', 'driver_id', 'team_id']\n",
    "page_lvl_cols = ['race_id']\n",
    "\n",
    "# Scrape fastest lap results\n",
    "df = scrape_url_table(\n",
    "    fastest_lap_urls,\n",
    "    total_cols,\n",
    "    col_idx_map,\n",
    "    id_cols,\n",
    "    page_lvl_cols=page_lvl_cols,\n",
    "    id_mask=constructor_mapping)\n",
    "df.to_csv('../data/raw/fastest_lap_results_raw.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924b539",
   "metadata": {},
   "source": [
    "# FastF1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00569f46",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb989bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85940535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FastF1 Emilia Romagna Troubleshooting ===\n",
      "\n",
      "1. Checking available events for 2021:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Error getting 2021 events: Failed to load any schedule data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "2. Checking Emilia Romagna across different years:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ 2020: Error - Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ 2021: Error - Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ 2022: Error - Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ 2023: Error - Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ 2024: Error - Failed to load any schedule data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "3. Testing with a known working session (2023 Bahrain):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Error with Bahrain 2023: Failed to load any schedule data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "4. Testing Emilia Romagna with different name variations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'Emilia Romagna' in 2021: Failed to load any schedule data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'Emilia-Romagna' in 2021: Failed to load any schedule data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'Imola' in 2021: Failed to load any schedule data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'San Marino' in 2021: Failed to load any schedule data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'Emilia Romagna Grand Prix' in 2021: Failed to load any schedule data....\n",
      "\n",
      "=== Troubleshooting Complete ===\n"
     ]
    }
   ],
   "source": [
    "# FastF1 Emilia Romagna Troubleshooting\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "\n",
    "# Enable cache\n",
    "fastf1.Cache.enable_cache('../data/cache')\n",
    "\n",
    "print(\"=== FastF1 Emilia Romagna Troubleshooting ===\\n\")\n",
    "\n",
    "# Test 1: Check what events are available for 2021\n",
    "print(\"1. Checking available events for 2021:\")\n",
    "try:\n",
    "    events_2021 = fastf1.get_event_schedule(2021)\n",
    "    print(f\"✓ Found {len(events_2021)} events in 2021\")\n",
    "    \n",
    "    # Look for Emilia Romagna or similar names\n",
    "    emilia_events = events_2021[events_2021['EventName'].str.contains('Emilia|Romagna|Imola', case=False, na=False)]\n",
    "    if not emilia_events.empty:\n",
    "        print(\"✓ Found Emilia Romagna related events:\")\n",
    "        for _, event in emilia_events.iterrows():\n",
    "            print(f\"  - {event['EventName']} ({event['EventDate']})\")\n",
    "    else:\n",
    "        print(\"✗ No Emilia Romagna events found in 2021\")\n",
    "        print(\"Available events in 2021:\")\n",
    "        for _, event in events_2021.iterrows():\n",
    "            print(f\"  - {event['EventName']} ({event['EventDate']})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error getting 2021 events: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 2: Try different years for Emilia Romagna\n",
    "print(\"\\n2. Checking Emilia Romagna across different years:\")\n",
    "years_to_test = [2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "for year in years_to_test:\n",
    "    try:\n",
    "        events = fastf1.get_event_schedule(year)\n",
    "        emilia_events = events[events['EventName'].str.contains('Emilia|Romagna|Imola', case=False, na=False)]\n",
    "        if not emilia_events.empty:\n",
    "            print(f\"✓ {year}: Found {len(emilia_events)} Emilia Romagna events\")\n",
    "            for _, event in emilia_events.iterrows():\n",
    "                print(f\"    - {event['EventName']} ({event['EventDate']})\")\n",
    "        else:\n",
    "            print(f\"✗ {year}: No Emilia Romagna events found\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {year}: Error - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 3: Try to get a working session first\n",
    "print(\"\\n3. Testing with a known working session (2023 Bahrain):\")\n",
    "try:\n",
    "    session = fastf1.get_session(2023, 'Bahrain', 'Race')\n",
    "    print(f\"✓ Successfully created session: {session}\")\n",
    "    \n",
    "    # Try to load data\n",
    "    session.load(laps=True, telemetry=False, weather=True, messages=True)\n",
    "    print(\"✓ Successfully loaded session data\")\n",
    "    \n",
    "    # Check data availability\n",
    "    print(f\"  - Laps: {len(session.laps) if hasattr(session, 'laps') and session.laps is not None else 'None'}\")\n",
    "    print(f\"  - Weather: {len(session.weather_data) if hasattr(session, 'weather_data') and session.weather_data is not None else 'None'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error with Bahrain 2023: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 4: Try Emilia Romagna with different naming variations\n",
    "print(\"\\n4. Testing Emilia Romagna with different name variations:\")\n",
    "name_variations = [\n",
    "    'Emilia Romagna',\n",
    "    'Emilia-Romagna', \n",
    "    'Imola',\n",
    "    'San Marino',  # Sometimes called this\n",
    "    'Emilia Romagna Grand Prix'\n",
    "]\n",
    "\n",
    "test_year = 2021\n",
    "for name in name_variations:\n",
    "    try:\n",
    "        session = fastf1.get_session(test_year, name, 'Race')\n",
    "        print(f\"✓ Success with '{name}' in {test_year}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed with '{name}' in {test_year}: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n=== Troubleshooting Complete ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7317be3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Potential Solutions ===\n",
      "\n",
      "1. NETWORK/CONNECTIVITY ISSUES:\n",
      "   - Check your internet connection\n",
      "   - Try running the troubleshooting cell above first\n",
      "   - The FastF1 API might be temporarily down\n",
      "\n",
      "2. YEAR/EVENT AVAILABILITY:\n",
      "   - 2021 Emilia Romagna might not have complete data\n",
      "   - Try 2022 or 2023 Emilia Romagna instead\n",
      "   - Some races have limited data availability\n",
      "\n",
      "3. NAME VARIATIONS:\n",
      "   - Try 'Imola' instead of 'Emilia Romagna'\n",
      "   - Try 'Emilia-Romagna' (with hyphen)\n",
      "   - FastF1 might use different naming conventions\n",
      "\n",
      "4. ALTERNATIVE APPROACH:\n",
      "   - Use a different race for testing (e.g., Bahrain 2023)\n",
      "   - Check if the issue is specific to Emilia Romagna\n",
      "   - Try with force_ergast=True parameter\n",
      "\n",
      "5. CACHE ISSUES:\n",
      "   - Clear the FastF1 cache: fastf1.Cache.clear_cache()\n",
      "   - Try without cache first\n",
      "\n",
      "6. FASTF1 VERSION:\n",
      "   - Update FastF1: pip install --upgrade fastf1\n",
      "   - Check if you're using a compatible version\n",
      "\n",
      "=== Testing with force_ergast=True ===\n",
      "✗ Still failed with force_ergast=True: Failed to load any schedule data.\n",
      "\n",
      "=== Testing with 'Imola' name ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 'Imola' name: Failed to load any schedule data.\n",
      "\n",
      "=== Testing with 2022 Emilia Romagna ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Failed with 2022 Emilia Romagna: Failed to load any schedule data.\n"
     ]
    }
   ],
   "source": [
    "# Potential Solutions for Emilia Romagna Issues\n",
    "\n",
    "print(\"=== Potential Solutions ===\\n\")\n",
    "\n",
    "print(\"1. NETWORK/CONNECTIVITY ISSUES:\")\n",
    "print(\"   - Check your internet connection\")\n",
    "print(\"   - Try running the troubleshooting cell above first\")\n",
    "print(\"   - The FastF1 API might be temporarily down\")\n",
    "\n",
    "print(\"\\n2. YEAR/EVENT AVAILABILITY:\")\n",
    "print(\"   - 2021 Emilia Romagna might not have complete data\")\n",
    "print(\"   - Try 2022 or 2023 Emilia Romagna instead\")\n",
    "print(\"   - Some races have limited data availability\")\n",
    "\n",
    "print(\"\\n3. NAME VARIATIONS:\")\n",
    "print(\"   - Try 'Imola' instead of 'Emilia Romagna'\")\n",
    "print(\"   - Try 'Emilia-Romagna' (with hyphen)\")\n",
    "print(\"   - FastF1 might use different naming conventions\")\n",
    "\n",
    "print(\"\\n4. ALTERNATIVE APPROACH:\")\n",
    "print(\"   - Use a different race for testing (e.g., Bahrain 2023)\")\n",
    "print(\"   - Check if the issue is specific to Emilia Romagna\")\n",
    "print(\"   - Try with force_ergast=True parameter\")\n",
    "\n",
    "print(\"\\n5. CACHE ISSUES:\")\n",
    "print(\"   - Clear the FastF1 cache: fastf1.Cache.clear_cache()\")\n",
    "print(\"   - Try without cache first\")\n",
    "\n",
    "print(\"\\n6. FASTF1 VERSION:\")\n",
    "print(\"   - Update FastF1: pip install --upgrade fastf1\")\n",
    "print(\"   - Check if you're using a compatible version\")\n",
    "\n",
    "# Quick test with force_ergast\n",
    "print(\"\\n=== Testing with force_ergast=True ===\")\n",
    "try:\n",
    "    session = fastf1.get_session(2021, 'Emilia Romagna', 'Race', force_ergast=True)\n",
    "    print(\"✓ Success with force_ergast=True\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Still failed with force_ergast=True: {e}\")\n",
    "\n",
    "# Test with Imola name\n",
    "print(\"\\n=== Testing with 'Imola' name ===\")\n",
    "try:\n",
    "    session = fastf1.get_session(2021, 'Imola', 'Race')\n",
    "    print(\"✓ Success with 'Imola' name\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed with 'Imola' name: {e}\")\n",
    "\n",
    "# Test with 2022\n",
    "print(\"\\n=== Testing with 2022 Emilia Romagna ===\")\n",
    "try:\n",
    "    session = fastf1.get_session(2022, 'Emilia Romagna', 'Race')\n",
    "    print(\"✓ Success with 2022 Emilia Romagna\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed with 2022 Emilia Romagna: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61386139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2018 Australia (original: Australia)...\n",
      "  Retry 1/3 for 2018 Australia FP1: Failed to load any schedule data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:498: UserWarning: Option ``force_ergast`` has been deprecated, use``backend='ergast'`` instead\n",
      "  warnings.warn(\"Option ``force_ergast`` has been deprecated, use\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retry 2/3 for 2018 Australia FP1: Failed to load any schedule data....\n",
      "  Failed after 3 retries for 2018 Australia FP1: Failed to load any schedule data....\n",
      "  Retry 1/3 for 2018 Australia FP2: Failed to load any schedule data....\n",
      "  Retry 2/3 for 2018 Australia FP2: Failed to load any schedule data....\n",
      "  Failed after 3 retries for 2018 Australia FP2: Failed to load any schedule data....\n",
      "  Retry 1/3 for 2018 Australia FP3: Failed to load any schedule data....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 56\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mfastf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m session\u001b[38;5;241m.\u001b[39mload(laps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, telemetry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weather\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:310\u001b[0m, in \u001b[0;36mget_session\u001b[1;34m(year, gp, identifier, backend, force_ergast)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a :class:`~fastf1.core.Session` object based on year, event name\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03mand session identifier.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m        from the ergast database to create the event schedule\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mget_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m event\u001b[38;5;241m.\u001b[39mget_session(identifier)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:405\u001b[0m, in \u001b[0;36mget_event\u001b[1;34m(year, gp, backend, force_ergast, strict_search, exact_match)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an :class:`~fastf1.events.Event` object for a specific\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03mseason and gp.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m.. versionadded:: 2.2\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[43mget_event_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_testing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gp, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:522\u001b[0m, in \u001b[0;36mget_event_schedule\u001b[1;34m(year, include_testing, backend, force_ergast)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# raise Error if fallback failed as well\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load any schedule data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_testing:\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load any schedule data.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     sleep_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m retry_count\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Failed after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m retries for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Improved FastF1 Data Collection with Better Error Handling\n",
    "\n",
    "# Initialize urls and sessions\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "sessions_collected = ['FP1', 'FP2', 'FP3', 'Qualifying', 'Race']\n",
    "\n",
    "# Enable FastF1 cache for better performance\n",
    "fastf1.Cache.enable_cache('../data/cache')\n",
    "\n",
    "# Suppress FastF1 logging output\n",
    "fastf1_logger = logging.getLogger('fastf1')\n",
    "fastf1_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Initialize empty DataFrames to collect all data\n",
    "all_laps = pd.DataFrame()\n",
    "all_weather = pd.DataFrame()\n",
    "all_messages = pd.DataFrame()\n",
    "\n",
    "# Grand Prix name corrections for FastF1\n",
    "gp_name_corrections = {\n",
    "    'Emilia Romagna': 'Imola',  # FastF1 often uses 'Imola' instead\n",
    "    'Emilia-Romagna': 'Imola',\n",
    "    'Great Britain': 'Great Britain',\n",
    "    'Saudi Arabia': 'Saudi Arabia',\n",
    "    'United States': 'United States',\n",
    "    'Abu Dhabi': 'Abu Dhabi'\n",
    "}\n",
    "\n",
    "def get_corrected_gp_name(gp_name):\n",
    "    \"\"\"Get the correct GP name for FastF1\"\"\"\n",
    "    return gp_name_corrections.get(gp_name, gp_name)\n",
    "\n",
    "successful_sessions = 0\n",
    "failed_sessions = 0\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    # Extract year and grand prix from the url\n",
    "    year = int(url.split('/')[5])\n",
    "    gp_raw = url.split('/')[8].replace('-', ' ').title()\n",
    "    gp = get_corrected_gp_name(gp_raw)\n",
    "    \n",
    "    print(f\"Processing {year} {gp} (original: {gp_raw})...\")\n",
    "    \n",
    "    for session_type in sessions_collected:\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                # Try with force_ergast=True as fallback\n",
    "                if retry_count > 0:\n",
    "                    session = fastf1.get_session(year, gp, session_type, force_ergast=True)\n",
    "                else:\n",
    "                    session = fastf1.get_session(year, gp, session_type)\n",
    "                \n",
    "                session.load(laps=True, telemetry=False, weather=True, messages=True)\n",
    "                success = True\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    sleep_time = 2 ** retry_count\n",
    "                    print(f'  Retry {retry_count}/{max_retries} for {year} {gp} {session_type}: {str(e)[:100]}...')\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(f'  Failed after {max_retries} retries for {year} {gp} {session_type}: {str(e)[:100]}...')\n",
    "                    failed_sessions += 1\n",
    "                    continue\n",
    "        \n",
    "        if not success:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Get race ID\n",
    "            race_id_map = load_id_map('../data/raw/race_id_map.pkl')\n",
    "            race_key = f\"{gp_raw}_{year}\"\n",
    "            if race_key in race_id_map:\n",
    "                race_id_value = race_id_map[race_key]\n",
    "            else:\n",
    "                print(f\"    No race_id found for: {race_key}\")\n",
    "                race_id_value = None\n",
    "            \n",
    "            # Process laps data\n",
    "            if hasattr(session, 'laps') and session.laps is not None and not session.laps.empty:\n",
    "                laps_df = session.laps.copy()\n",
    "                laps_df['race_id'] = race_id_value\n",
    "                laps_df['session'] = session_type\n",
    "                all_laps = pd.concat([all_laps, laps_df], ignore_index=True)\n",
    "            \n",
    "            # Process weather data\n",
    "            if hasattr(session, 'weather_data') and session.weather_data is not None:\n",
    "                weather_df = pd.DataFrame(session.weather_data)\n",
    "                weather_df['race_id'] = race_id_value\n",
    "                weather_df['session'] = session_type\n",
    "                all_weather = pd.concat([all_weather, weather_df], ignore_index=True)\n",
    "            \n",
    "            # Process messages data\n",
    "            if hasattr(session, 'race_control_messages') and session.race_control_messages is not None:\n",
    "                messages_df = pd.DataFrame(session.race_control_messages)\n",
    "                messages_df['race_id'] = race_id_value\n",
    "                messages_df['session'] = session_type\n",
    "                all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n",
    "            \n",
    "            successful_sessions += 1\n",
    "            print(f\"    ✓ Successfully loaded {session_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing data for {year} {gp} {session_type}: {e}\")\n",
    "            failed_sessions += 1\n",
    "\n",
    "print(f\"\\n=== Data Collection Complete ===\")\n",
    "print(f\"Successful sessions: {successful_sessions}\")\n",
    "print(f\"Failed sessions: {failed_sessions}\")\n",
    "\n",
    "# Save final DataFrames to CSVs\n",
    "if not all_laps.empty:\n",
    "    all_laps.to_csv('../data/raw/lap_data_raw.csv', index=False)\n",
    "    print(f\"Saved {len(all_laps)} lap records to lap_data_raw.csv\")\n",
    "else:\n",
    "    print(\"No lap data to save\")\n",
    "\n",
    "if not all_weather.empty:\n",
    "    all_weather.to_csv('../data/raw/weather_data_raw.csv', index=False)\n",
    "    print(f\"Saved {len(all_weather)} weather records to weather_data_raw.csv\")\n",
    "else:\n",
    "    print(\"No weather data to save\")\n",
    "\n",
    "if not all_messages.empty:\n",
    "    all_messages.to_csv('../data/raw/messages_data_raw.csv', index=False)\n",
    "    print(f\"Saved {len(all_messages)} message records to messages_data_raw.csv\")\n",
    "else:\n",
    "    print(\"No message data to save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2783c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef83ad",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d008f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c523242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/3 for (2018, 'Australia', 'FP1'): Failed to load any schedule data.. Sleeping for 2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 2/3 for (2018, 'Australia', 'FP1'): Failed to load any schedule data.. Sleeping for 4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed after 3 retries for (2018, 'Australia', 'FP1'): Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/3 for (2018, 'Australia', 'FP2'): Failed to load any schedule data.. Sleeping for 2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 2/3 for (2018, 'Australia', 'FP2'): Failed to load any schedule data.. Sleeping for 4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed after 3 retries for (2018, 'Australia', 'FP2'): Failed to load any schedule data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n",
      "logger      WARNING \tFailed to load schedule from F1 API backend!\n",
      "logger      WARNING \tFailed to load schedule from Ergast API backend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/3 for (2018, 'Australia', 'FP3'): Failed to load any schedule data.. Sleeping for 2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger      WARNING \tFailed to load schedule from FastF1 backend!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m max_retries \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         session \u001b[38;5;241m=\u001b[39m \u001b[43mfastf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m         session\u001b[38;5;241m.\u001b[39mload(laps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, telemetry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weather\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m         weather_array \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mweather_data\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:310\u001b[0m, in \u001b[0;36mget_session\u001b[1;34m(year, gp, identifier, backend, force_ergast)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_session\u001b[39m(\n\u001b[0;32m    243\u001b[0m         year: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    244\u001b[0m         gp: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m         force_ergast: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Session:\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a :class:`~fastf1.core.Session` object based on year, event name\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    and session identifier.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m            from the ergast database to create the event schedule\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[43mget_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event\u001b[38;5;241m.\u001b[39mget_session(identifier)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:405\u001b[0m, in \u001b[0;36mget_event\u001b[1;34m(year, gp, backend, force_ergast, strict_search, exact_match)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_event\u001b[39m(\n\u001b[0;32m    351\u001b[0m         year: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    352\u001b[0m         gp: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m         exact_match: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an :class:`~fastf1.events.Event` object for a specific\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    season and gp.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.2\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m     schedule \u001b[38;5;241m=\u001b[39m \u001b[43mget_event_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_testing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    410\u001b[0m         event \u001b[38;5;241m=\u001b[39m schedule\u001b[38;5;241m.\u001b[39mget_event_by_name(\n\u001b[0;32m    411\u001b[0m             gp, strict_search\u001b[38;5;241m=\u001b[39mstrict_search, exact_match\u001b[38;5;241m=\u001b[39mexact_match)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:517\u001b[0m, in \u001b[0;36mget_event_schedule\u001b[1;34m(year, include_testing, backend, force_ergast)\u001b[0m\n\u001b[0;32m    515\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _backends:\n\u001b[1;32m--> 517\u001b[0m     schedule \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\logger.py:151\u001b[0m, in \u001b[0;36msoft_exceptions.<locals>.__decorator.<locals>.__wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LoggingManager\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    153\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(msg)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:659\u001b[0m, in \u001b[0;36m_get_schedule_from_f1_timing\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;129m@soft_exceptions\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 API schedule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    655\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load schedule from F1 API backend!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    656\u001b[0m                  _logger)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_schedule_from_f1_timing\u001b[39m(year: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# create an event schedule using data from the F1 API\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfastf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseason_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/static/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     data \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\req.py:479\u001b[0m, in \u001b[0;36mCache.api_request_wrapper.<locals>._cached_api_request\u001b[1;34m(api_path, **func_kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# cached data does not yet exist for this api request\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo cached data found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    478\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 479\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_write_cache(data, cache_file_path)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\_api.py:1678\u001b[0m, in \u001b[0;36mseason_schedule\u001b[1;34m(path, response)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1677\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching season schedule...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1678\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# no response received\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SessionNotAvailableError(\n\u001b[0;32m   1681\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data for this session! If this session only finished \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1682\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecently, please try again in a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1683\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\_api.py:1752\u001b[0m, in \u001b[0;36mfetch_page\u001b[1;34m(path, name)\u001b[0m\n\u001b[0;32m   1749\u001b[0m is_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjsonStream\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m page\n\u001b[0;32m   1750\u001b[0m is_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.z.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m page\n\u001b[1;32m-> 1752\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m   1755\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to livetiming mirror (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url_mirror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\req.py:303\u001b[0m, in \u001b[0;36mCache.requests_get\u001b[1;34m(cls, url, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_request_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\req.py:347\u001b[0m, in \u001b[0;36mCache._cached_request\u001b[1;34m(cls, method, url, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# catch TypeError raised by outdated requests-cache version if the\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# cache was created with a newer version\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# github.com/requests-cache/requests-cache/issues/973\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using an outdated version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests-cache. Consider upgrading.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests_cache\\session.py:127\u001b[0m, in \u001b[0;36mCacheMixin.get\u001b[1;34m(self, url, params, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AnyResponse:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests_cache\\session.py:183\u001b[0m, in \u001b[0;36mCacheMixin.request\u001b[1;34m(self, method, url, headers, expire_after, only_if_cached, refresh, force_refresh, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m headers \u001b[38;5;241m=\u001b[39m set_request_headers(headers, expire_after, only_if_cached, refresh, force_refresh)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch_form_boundary() \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests_cache\\session.py:230\u001b[0m, in \u001b[0;36mCacheMixin.send\u001b[1;34m(self, request, expire_after, only_if_cached, refresh, force_refresh, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resend(request, actions, cached_response, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m actions\u001b[38;5;241m.\u001b[39msend_request:\n\u001b[1;32m--> 230\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_and_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     response \u001b[38;5;241m=\u001b[39m cached_response  \u001b[38;5;66;03m# type: ignore  # Guaranteed to be non-None by this point\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests_cache\\session.py:254\u001b[0m, in \u001b[0;36mCacheMixin._send_and_cache\u001b[1;34m(self, request, actions, cached_response, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a request and cache the response, unless disabled by settings or headers.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03mIf applicable, also handle conditional requests.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m request \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mupdate_request(request)\n\u001b[1;32m--> 254\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m actions\u001b[38;5;241m.\u001b[39mupdate_from_response(response)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mskip_write:\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\req.py:134\u001b[0m, in \u001b[0;36m_SessionWithRateLimiting.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pattern\u001b[38;5;241m.\u001b[39mmatch(request\u001b[38;5;241m.\u001b[39murl):\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lim \u001b[38;5;129;01min\u001b[39;00m limiters:\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;66;03m# apply all defined limiters\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m             \u001b[43mlim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\req.py:83\u001b[0m, in \u001b[0;36m_MinIntervalLimitDelay.limit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m t_now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (delta \u001b[38;5;241m:=\u001b[39m (t_now \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_t_last)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interval:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     t_now \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interval \u001b[38;5;241m-\u001b[39m delta\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_t_last \u001b[38;5;241m=\u001b[39m t_now\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize urls and sessions\n",
    "urls = load_id_map('../data/raw/links_2018+.pkl')\n",
    "sessions_collected = ['FP1', 'FP2', 'FP3', 'Qualifying', 'Race']\n",
    "os.makedirs('../data/cache', exist_ok=True)\n",
    "fastf1.Cache.enable_cache('../data/cache')\n",
    "\n",
    "# Suppress FastF1 logging output\n",
    "fastf1_logger = logging.getLogger('fastf1')\n",
    "fastf1_logger.setLevel(logging.NOTSET)\n",
    "\n",
    "# Initialize empty DataFrames to collect all data\n",
    "all_laps = pd.DataFrame()\n",
    "all_weather = pd.DataFrame()\n",
    "all_messages = pd.DataFrame()\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    # Sort year and grand prix from the url\n",
    "    year = int(url.split('/')[5])\n",
    "    gp = url.split('/')[8].replace('-', ' ').title().replace('Emilia Romagna', 'Emilia-Romagna')\n",
    "    \n",
    "    for s in sessions_collected:\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                session = fastf1.get_session(year, gp, s)\n",
    "                session.load(laps=True, telemetry=False, weather=True, messages=True)\n",
    "                weather_array = session.weather_data\n",
    "                messages_array = session.race_control_messages\n",
    "\n",
    "                # Convert arrays into DataFrames\n",
    "                laps_df = session.laps\n",
    "                weather_df = pd.DataFrame(weather_array)\n",
    "                messages_df = pd.DataFrame(messages_array)\n",
    "                success = True\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    sleep_time = 2 ** retry_count\n",
    "                    print(f'Retry {retry_count}/{max_retries} for {year, gp, s}: {e}. Sleeping for {sleep_time}s...')\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(f'Failed after {max_retries} retries for {year, gp, s}: {e}')\n",
    "                    continue\n",
    "        \n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Get race ID\n",
    "        race_id_map = load_id_map('../data/raw/race_id_map.pkl')\n",
    "        race_key = f\"{gp}_{year}\"\n",
    "        if race_key in race_id_map:\n",
    "            race_id_value = race_id_map[race_key]\n",
    "        else:\n",
    "            print(f\"No race_id found for: {race_key}\")\n",
    "            race_id_value = None\n",
    "        \n",
    "        # Add race_id and session columns to each DataFrame and merge\n",
    "        if 'laps_df' in locals():\n",
    "            laps_df['race_id'] = race_id_value\n",
    "            laps_df['session'] = s\n",
    "            all_laps = pd.concat([all_laps, laps_df], ignore_index=True)\n",
    "        if 'weather_df' in locals():\n",
    "            weather_df['race_id'] = race_id_value\n",
    "            weather_df['session'] = s\n",
    "            all_weather = pd.concat([all_weather, weather_df], ignore_index=True)\n",
    "        if 'messages_df' in locals():\n",
    "            messages_df['race_id'] = race_id_value\n",
    "            messages_df['session'] = s\n",
    "            all_messages = pd.concat([all_messages, messages_df], ignore_index=True)\n",
    "\n",
    "# Save final DataFrames to CSVs\n",
    "all_laps.to_csv('../data/raw/lap_data_raw.csv', index=False)\n",
    "all_weather.to_csv('../data/raw/weather_data_raw.csv', index=False)\n",
    "all_messages.to_csv('../data/raw/messages_data_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34c689f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to load any schedule data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m fastf1\u001b[38;5;241m.\u001b[39mCache\u001b[38;5;241m.\u001b[39menable_cache(cache_dir)  \u001b[38;5;66;03m# uses the created \"cache\" folder to store data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load a session: example Bahrain GP 2023 Qualifying\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mfastf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBahrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m session\u001b[38;5;241m.\u001b[39mload(weather\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# only load weather data as requested\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Weather data is stored in session.weather_data (a structured numpy array)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:310\u001b[0m, in \u001b[0;36mget_session\u001b[1;34m(year, gp, identifier, backend, force_ergast)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_session\u001b[39m(\n\u001b[0;32m    243\u001b[0m         year: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    244\u001b[0m         gp: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m         force_ergast: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Session:\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a :class:`~fastf1.core.Session` object based on year, event name\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    and session identifier.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m            from the ergast database to create the event schedule\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[43mget_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event\u001b[38;5;241m.\u001b[39mget_session(identifier)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:405\u001b[0m, in \u001b[0;36mget_event\u001b[1;34m(year, gp, backend, force_ergast, strict_search, exact_match)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_event\u001b[39m(\n\u001b[0;32m    351\u001b[0m         year: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    352\u001b[0m         gp: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m         exact_match: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an :class:`~fastf1.events.Event` object for a specific\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    season and gp.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.2\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m     schedule \u001b[38;5;241m=\u001b[39m \u001b[43mget_event_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_testing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mforce_ergast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ergast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    410\u001b[0m         event \u001b[38;5;241m=\u001b[39m schedule\u001b[38;5;241m.\u001b[39mget_event_by_name(\n\u001b[0;32m    411\u001b[0m             gp, strict_search\u001b[38;5;241m=\u001b[39mstrict_search, exact_match\u001b[38;5;241m=\u001b[39mexact_match)\n",
      "File \u001b[1;32mc:\\Users\\jackw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastf1\\events.py:522\u001b[0m, in \u001b[0;36mget_event_schedule\u001b[1;34m(year, include_testing, backend, force_ergast)\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# raise Error if fallback failed as well\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load any schedule data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_testing:\n\u001b[0;32m    525\u001b[0m     schedule \u001b[38;5;241m=\u001b[39m schedule[\u001b[38;5;241m~\u001b[39mschedule\u001b[38;5;241m.\u001b[39mis_testing()]\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load any schedule data."
     ]
    }
   ],
   "source": [
    "# Enable cache (important for performance)\n",
    "# First ensure the cache directory exists\n",
    "import os\n",
    "cache_dir = \"cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    print(f\"Created cache directory: {cache_dir}\")\n",
    "\n",
    "fastf1.Cache.enable_cache(cache_dir)  # uses the created \"cache\" folder to store data\n",
    "\n",
    "# Load a session: example Bahrain GP 2023 Qualifying\n",
    "session = fastf1.get_session(2023, 'Bahrain', 'race')\n",
    "session.load(weather=True)  # only load weather data as requested\n",
    "\n",
    "# Weather data is stored in session.weather_data (a structured numpy array)\n",
    "weather_array = session.weather_data\n",
    "\n",
    "# Convert weather data to DataFrame\n",
    "weather_df = pd.DataFrame(weather_array)\n",
    "\n",
    "# Display weather data\n",
    "weather_df\n",
    "\n",
    "# Save weather dataframe to CSV file\n",
    "weather_df.to_csv(\"example_weather.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0995c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirTemp_mean        15.707865\n",
      "AirTemp_min              15.1\n",
      "AirTemp_max              16.6\n",
      "AirTemp_std           0.37574\n",
      "TrackTemp_mean      18.942135\n",
      "TrackTemp_min            18.3\n",
      "TrackTemp_max            19.4\n",
      "TrackTemp_std        0.276315\n",
      "WindSpeed_mean       3.475281\n",
      "WindSpeed_min             0.7\n",
      "WindSpeed_max             6.9\n",
      "WindSpeed_std        1.242267\n",
      "Humidity_mean       78.421348\n",
      "Humidity_min             68.0\n",
      "Humidity_max             92.0\n",
      "Humidity_std          6.50658\n",
      "Pressure_mean     1009.901685\n",
      "Pressure_min           1009.0\n",
      "Pressure_max           1010.7\n",
      "Pressure_std         0.444994\n",
      "Rainfall_any             True\n",
      "Rainfall_mean        0.325843\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example with weather data\n",
    "numeric_columns = ['AirTemp', 'TrackTemp', 'WindSpeed', 'Humidity', 'Pressure']\n",
    "boolean_columns = ['Rainfall']\n",
    "\n",
    "session_weather_features = aggregate_columns(\n",
    "    weather_df, \n",
    "    columns=numeric_columns, \n",
    "    boolean_columns=boolean_columns\n",
    ")\n",
    "\n",
    "print(session_weather_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
